{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running PatchTST-FM on gift-eval benchmark\n",
    "**The following notebook is only intended to reproduce GIFT-Eval results.**\n",
    "Make sure you download the gift-eval benchmark and set the `GIFT_EVAL` environment variable correctly before running this notebook.\n",
    "We will use the `Dataset` class to load the data and run the model. If you have not already please check out the [dataset.ipynb](./dataset.ipynb) notebook to learn more about the `Dataset` class. We are going to just run the model on two datasets for brevity. But feel free to run on any dataset by changing the `short_datasets` and `med_long_datasets` variables below.\n",
    "\n",
    "Please note: The submitted gift-eval results were generated using the following hardware and library versions:\n",
    "- GPU Model: NVIDIA RTX Pro 6000 Blackwell 96GB\n",
    "- CUDA Driver: 570.270, CUDA 12.8\n",
    "- torch: 2.8.0+cu129\n",
    "- transformers: 4.53.3\n",
    "\n",
    "Similar results were also obtained when using an NVIDIA A100-SXM4 80GB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSFM and TTM Installation\n",
    "1. Clone the [GIFT-Eval repository](https://github.com/SalesforceAIResearch/gift-eval).\n",
    "1. Follow the instruction to set up the GIFT-Eval environment as described [here](https://github.com/SalesforceAIResearch/gift-eval?tab=readme-ov-file#installation).\n",
    "1. This notebook should be placed in the `notebooks` folder of the cloned repository.\n",
    "1. Follow the instructions below to install TSFM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing `granite-tsfm`\n",
    "The source code will be installed from the [granite-tsfm repository](https://github.com/ibm-granite/granite-tsfm).\n",
    "Run the following code once to install granite-tsfm in your working python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "if not os.path.exists(\"granite-tsfm\"):\n",
    "    ! git clone --branch patchtst-fm git@github.com:ibm-granite/granite-tsfm.git\n",
    "    %cd granite-tsfm\n",
    "    ! pwd\n",
    "    # Switch to the desired branch\n",
    "    ! pip install \".[notebooks]\"\n",
    "    %cd ..\n",
    "else:\n",
    "    print(\"Folder 'granite-tsfm' already exists. Skipping git clone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from gift_eval.data import Dataset\n",
    "from gluonts.ev.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "from gluonts.model import evaluate_forecasts\n",
    "from gluonts.time_feature import get_seasonality, norm_freq_str\n",
    "\n",
    "from tsfm_public import PatchTSTFMForPrediction\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Update the python path to include the custom eval predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"./granite-tsfm/notebooks/hfdemo/patchtst_fm/\")\n",
    "\n",
    "from patchtst_fm_predictor import PatchTSTFMEvalPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Set up loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarningFilter(logging.Filter):\n",
    "    def __init__(self, text_to_filter):\n",
    "        super().__init__()\n",
    "        self.text_to_filter = text_to_filter\n",
    "\n",
    "    def filter(self, record):\n",
    "        return self.text_to_filter not in record.getMessage()\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "gts_logger = logging.getLogger(\"gluonts.model.forecast\")\n",
    "gts_logger.addFilter(\n",
    "    WarningFilter(\"The mean prediction is not stored in the forecast data\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Dataset and metrics configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short_datasets = \"m4_yearly m4_quarterly m4_monthly m4_weekly m4_daily m4_hourly electricity/15T electricity/H electricity/D electricity/W solar/10T solar/H solar/D solar/W hospital covid_deaths us_births/D us_births/M us_births/W saugeenday/D saugeenday/M saugeenday/W temperature_rain_with_missing kdd_cup_2018_with_missing/H kdd_cup_2018_with_missing/D car_parts_with_missing restaurant hierarchical_sales/D hierarchical_sales/W LOOP_SEATTLE/5T LOOP_SEATTLE/H LOOP_SEATTLE/D SZ_TAXI/15T SZ_TAXI/H M_DENSE/H M_DENSE/D ett1/15T ett1/H ett1/D ett1/W ett2/15T ett2/H ett2/D ett2/W jena_weather/10T jena_weather/H jena_weather/D bitbrains_fast_storage/5T bitbrains_fast_storage/H bitbrains_rnd/5T bitbrains_rnd/H bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "# med_long_datasets = \"electricity/15T electricity/H solar/10T solar/H kdd_cup_2018_with_missing/H LOOP_SEATTLE/5T LOOP_SEATTLE/H SZ_TAXI/15T M_DENSE/H ett1/15T ett1/H ett2/15T ett2/H jena_weather/10T jena_weather/H bitbrains_fast_storage/5T bitbrains_rnd/5T bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "short_datasets = \"m4_weekly\"\n",
    "med_long_datasets = \"bizitobs_l2c/H\"\n",
    "\n",
    "all_datasets = list(set(short_datasets.split() + med_long_datasets.split()))\n",
    "pretty_names = {\n",
    "    \"saugeenday\": \"saugeen\",\n",
    "    \"temperature_rain_with_missing\": \"temperature_rain\",\n",
    "    \"kdd_cup_2018_with_missing\": \"kdd_cup_2018\",\n",
    "    \"car_parts_with_missing\": \"car_parts\",\n",
    "}\n",
    "\n",
    "dataset_properties_map = json.load(open(\"dataset_properties.json\"))\n",
    "\n",
    "# Instantiate the metrics\n",
    "metrics = [\n",
    "    MSE(forecast_type=\"mean\"),\n",
    "    MSE(forecast_type=0.5),\n",
    "    MAE(),\n",
    "    MASE(),\n",
    "    MAPE(),\n",
    "    SMAPE(),\n",
    "    MSIS(),\n",
    "    RMSE(),\n",
    "    NRMSE(),\n",
    "    ND(),\n",
    "    MeanWeightedSumQuantileLoss(\n",
    "        quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "model_name = \"PatchTST-FM-r1\"\n",
    "ckpt_path = \"ibm-research/patchtst-fm-r1\"\n",
    "output_dir = f\"../results/{model_name}\"\n",
    "\n",
    "# Define the path for the results CSV file\n",
    "csv_file_path = os.path.join(output_dir, \"all_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "\n",
    "Now that we have defined imported tsfm-specific classes and configured the metrics and output paths, we will evaluate a PatchTSTFM model on the gift-eval benchmark datasets.\n",
    "Below, we will load the PatchTST-FM zero-shot model and then iterate through all the datasets, evaluating on each using the PatchTSTFMEvalPredictor.\n",
    "\n",
    "We are going to follow the naming conventions explained in the\n",
    "[README](../README.md) file to store the results in a csv file\n",
    "called `all_results.csv` under the `results/PatchTST-FM-r1` folder.\n",
    "\n",
    "The first column in the csv file is the dataset config name which\n",
    "is a combination of the dataset name, frequency and the term:\n",
    "\n",
    "```python\n",
    "f\"{dataset_name}/{freq}/{term}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"Loading model from {ckpt_path}\")\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else (\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    ")\n",
    "model = PatchTSTFMForPrediction.from_pretrained(ckpt_path, device_map=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for ds_num, ds_name in enumerate(all_datasets):\n",
    "    ds_key = ds_name.split(\"/\")[0]\n",
    "    logger.info(f\"Processing dataset: {ds_name} ({ds_num + 1} of {len(all_datasets)})\")\n",
    "    terms = [\"short\", \"medium\", \"long\"]\n",
    "\n",
    "    for term in terms:\n",
    "        if (\n",
    "            term == \"medium\" or term == \"long\"\n",
    "        ) and ds_name not in med_long_datasets.split():\n",
    "            continue\n",
    "\n",
    "        if \"/\" in ds_name:\n",
    "            ds_key = ds_name.split(\"/\")[0]\n",
    "            ds_freq = ds_name.split(\"/\")[1]\n",
    "            ds_key = ds_key.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "        else:\n",
    "            ds_key = ds_name.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "            ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "        ds_config = f\"{ds_key}/{ds_freq}/{term}\"\n",
    "\n",
    "        logger.info(f\"config: {ds_config}\")\n",
    "        # Initialize the dataset\n",
    "        to_univariate = (\n",
    "            False\n",
    "            if Dataset(name=ds_name, term=term, to_univariate=False).target_dim == 1\n",
    "            else True\n",
    "        )\n",
    "        dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate)\n",
    "        # target_dim = Dataset(name=ds_name, term=term, to_univariate=False).target_dim\n",
    "        # dataset = Dataset(name=ds_name, term=term, to_univariate=target_dim != 1)\n",
    "        season_length = get_seasonality(dataset.freq)\n",
    "        logger.info(f\"Dataset size: {len(dataset.test_data)}\")\n",
    "\n",
    "        predictor = PatchTSTFMEvalPredictor(\n",
    "            model=model,\n",
    "            prediction_length=dataset.prediction_length,\n",
    "            dataset_name=ds_name,\n",
    "            quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "        )\n",
    "\n",
    "        forecasts = predictor.predict(\n",
    "            dataset.test_data.input,\n",
    "            batch_size=2048,\n",
    "        )\n",
    "\n",
    "        res = (\n",
    "            evaluate_forecasts(\n",
    "                forecasts,\n",
    "                test_data=dataset.test_data,\n",
    "                metrics=metrics,\n",
    "                axis=None,\n",
    "                batch_size=1024,\n",
    "                mask_invalid_label=True,\n",
    "                allow_nan_forecast=False,\n",
    "                seasonality=season_length,\n",
    "            )\n",
    "            .reset_index(drop=True)\n",
    "            .to_dict(orient=\"records\")\n",
    "        )\n",
    "\n",
    "        all_results.append(\n",
    "            (\n",
    "                res,\n",
    "                ds_config,\n",
    "                dataset_properties_map[ds_key][\"domain\"],\n",
    "                dataset_properties_map[ds_key][\"num_variates\"],\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize results and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_rows = []\n",
    "for result_metrics, ds_config, domain, num_variates in all_results:\n",
    "    result_metrics = {f\"eval_metrics/{k}\": v for k, v in result_metrics[0].items()}\n",
    "\n",
    "    result_df_rows.append(\n",
    "        {\n",
    "            \"dataset\": ds_config,\n",
    "            \"model\": model_name,\n",
    "            **result_metrics,\n",
    "            \"domain\": domain,\n",
    "            \"num_variates\": num_variates,\n",
    "        }\n",
    "    )\n",
    "results_df = pd.DataFrame(result_df_rows).sort_values(by=\"dataset\")\n",
    "results_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "logger.info(f\"Results have been written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "df = pd.read_csv(f\"{output_dir}/all_results.csv\")\n",
    "df = df.sort_values(by=\"dataset\")\n",
    "display(\n",
    "    df[\n",
    "        [\n",
    "            \"dataset\",\n",
    "            \"eval_metrics/MASE[0.5]\",\n",
    "            \"eval_metrics/NRMSE[mean]\",\n",
    "            \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
