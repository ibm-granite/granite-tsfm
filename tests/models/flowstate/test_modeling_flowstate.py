# Copyright contributors to the TSFM project
#
"""Testing suite for the PyTorch FlowState model."""

# import torchinfo
import unittest

import torch
from parameterized import parameterized

from tsfm_public.models.flowstate.configuration_flowstate import FlowStateConfig
from tsfm_public.models.flowstate.modeling_flowstate import (
    FlowStateForPrediction,
    FlowStateForPredictionOutput,
    FlowStateModel,
    FlowStateModelOutput,
)


TOLERANCE = 1e-4


class FlowStateFunctionalTests(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        """Setup method: Called once before test-cases execution"""
        cls.params = {}
        cls.params.update(
            context_length=36,
            batch_first=True,
            scale_factor=1.0,
            prediction_length=12,
            # Embedding specific configuration
            embedding_feature_dim=5,
            # Encoder specific configuration
            encoder_num_layers=3,
            encoder_state_dim=8,
            encoder_num_hippo_blocks=4,
            # Decoder specific configuration
            decoder_patch_len=5,
            decoder_dim=6,
            decoder_type="legs",
            # Loss function / Prediction
            quantiles=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
            prediction_type="quantile",
            num_input_channels=1,
        )

        batch_size = 2
        cls.batch_size = batch_size

        cls.constant_data = torch.ones(
            batch_size,
            cls.params["context_length"],
            cls.params["num_input_channels"],
        )
        cls.constant_data = (
            cls.constant_data if cls.params["batch_first"] else torch.transpose(cls.constant_data, 1, 0)
        )

        # cls.embed_output = torch.rand(
        #     cls.params["context_length"],
        #     batch_size,
        #     cls.params["embedding_feature_dim"],
        # )
        cls.embed_output = torch.tensor(
            [
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
                [[0.6146, 0.1323, 0.5224, 0.0958, 0.3410], [0.6146, 0.1323, 0.5224, 0.0958, 0.3410]],
            ]
        )

        # cls.enc_output_hidden_states = [torch.rand(
        #     cls.params["context_length"],
        #     batch_size,
        #     cls.params["encoder_state_dim"],
        # ) for _ in range(cls.params["encoder_num_layers"] - 1)] + \
        # [torch.rand(
        #     1,
        #     batch_size,
        #     cls.params["encoder_state_dim"],
        # )]
        cls.enc_output_hidden_states = [
            torch.tensor(
                [
                    [[1.3950, 1.2677, 0.3756, 0.0789, -1.4111], [1.3950, 1.2677, 0.3756, 0.0789, -1.4111]],
                    [[1.4152, 1.2659, 0.3814, 0.0466, -1.4031], [1.4152, 1.2659, 0.3814, 0.0466, -1.4031]],
                    [[1.4319, 1.2655, 0.3874, 0.0164, -1.3951], [1.4319, 1.2655, 0.3874, 0.0164, -1.3951]],
                    [[1.4405, 1.2690, 0.3947, -0.0102, -1.3879], [1.4405, 1.2690, 0.3947, -0.0102, -1.3879]],
                    [[1.4488, 1.2722, 0.4015, -0.0357, -1.3806], [1.4488, 1.2722, 0.4015, -0.0357, -1.3806]],
                    [[1.4568, 1.2752, 0.4077, -0.0603, -1.3732], [1.4568, 1.2752, 0.4077, -0.0603, -1.3732]],
                    [[1.4644, 1.2779, 0.4133, -0.0838, -1.3657], [1.4644, 1.2779, 0.4133, -0.0838, -1.3657]],
                    [[1.4716, 1.2805, 0.4185, -0.1063, -1.3582], [1.4716, 1.2805, 0.4185, -0.1063, -1.3582]],
                    [[1.4785, 1.2830, 0.4231, -0.1278, -1.3507], [1.4785, 1.2830, 0.4231, -0.1278, -1.3507]],
                    [[1.4850, 1.2853, 0.4273, -0.1484, -1.3432], [1.4850, 1.2853, 0.4273, -0.1484, -1.3432]],
                    [[1.4913, 1.2875, 0.4310, -0.1679, -1.3358], [1.4913, 1.2875, 0.4310, -0.1679, -1.3358]],
                    [[1.4971, 1.2897, 0.4342, -0.1866, -1.3284], [1.4971, 1.2897, 0.4342, -0.1866, -1.3284]],
                    [[1.5026, 1.2918, 0.4371, -0.2044, -1.3210], [1.5026, 1.2918, 0.4371, -0.2044, -1.3210]],
                    [[1.5078, 1.2939, 0.4395, -0.2213, -1.3138], [1.5078, 1.2939, 0.4395, -0.2213, -1.3138]],
                    [[1.5127, 1.2959, 0.4415, -0.2373, -1.3067], [1.5127, 1.2959, 0.4415, -0.2373, -1.3067]],
                    [[1.5173, 1.2980, 0.4432, -0.2526, -1.2997], [1.5173, 1.2980, 0.4432, -0.2526, -1.2997]],
                    [[1.5215, 1.3001, 0.4445, -0.2671, -1.2929], [1.5215, 1.3001, 0.4445, -0.2671, -1.2929]],
                    [[1.5255, 1.3021, 0.4454, -0.2808, -1.2861], [1.5255, 1.3021, 0.4454, -0.2808, -1.2861]],
                    [[1.5291, 1.3043, 0.4461, -0.2939, -1.2795], [1.5291, 1.3043, 0.4461, -0.2939, -1.2795]],
                    [[1.5325, 1.3065, 0.4464, -0.3062, -1.2730], [1.5325, 1.3065, 0.4464, -0.3062, -1.2730]],
                    [[1.5355, 1.3087, 0.4465, -0.3180, -1.2667], [1.5355, 1.3087, 0.4465, -0.3180, -1.2667]],
                    [[1.5383, 1.3110, 0.4463, -0.3291, -1.2605], [1.5383, 1.3110, 0.4463, -0.3291, -1.2605]],
                    [[1.5408, 1.3134, 0.4458, -0.3396, -1.2545], [1.5408, 1.3134, 0.4458, -0.3396, -1.2545]],
                    [[1.5431, 1.3159, 0.4452, -0.3495, -1.2485], [1.5431, 1.3159, 0.4452, -0.3495, -1.2485]],
                    [[1.5450, 1.3185, 0.4442, -0.3589, -1.2428], [1.5450, 1.3185, 0.4442, -0.3589, -1.2428]],
                    [[1.5467, 1.3212, 0.4431, -0.3678, -1.2371], [1.5467, 1.3212, 0.4431, -0.3678, -1.2371]],
                    [[1.5482, 1.3240, 0.4418, -0.3762, -1.2316], [1.5482, 1.3240, 0.4418, -0.3762, -1.2316]],
                    [[1.5493, 1.3269, 0.4403, -0.3842, -1.2262], [1.5493, 1.3269, 0.4403, -0.3842, -1.2262]],
                    [[1.5503, 1.3299, 0.4386, -0.3917, -1.2209], [1.5503, 1.3299, 0.4386, -0.3917, -1.2209]],
                    [[1.5510, 1.3330, 0.4368, -0.3988, -1.2158], [1.5510, 1.3330, 0.4368, -0.3988, -1.2158]],
                    [[1.5514, 1.3362, 0.4348, -0.4056, -1.2108], [1.5514, 1.3362, 0.4348, -0.4056, -1.2108]],
                    [[1.5516, 1.3396, 0.4326, -0.4119, -1.2058], [1.5516, 1.3396, 0.4326, -0.4119, -1.2058]],
                    [[1.5516, 1.3430, 0.4304, -0.4179, -1.2010], [1.5516, 1.3430, 0.4304, -0.4179, -1.2010]],
                    [[1.5514, 1.3466, 0.4280, -0.4236, -1.1963], [1.5514, 1.3466, 0.4280, -0.4236, -1.1963]],
                    [[1.5509, 1.3503, 0.4255, -0.4290, -1.1916], [1.5509, 1.3503, 0.4255, -0.4290, -1.1916]],
                    [[1.5502, 1.3542, 0.4229, -0.4341, -1.1871], [1.5502, 1.3542, 0.4229, -0.4341, -1.1871]],
                ]
            ),
            torch.tensor(
                [
                    [[1.9439, 2.8983, -0.2151, -0.2526, -2.6684], [1.9439, 2.8983, -0.2151, -0.2526, -2.6684]],
                    [[1.9769, 2.8930, -0.2087, -0.2995, -2.6556], [1.9769, 2.8930, -0.2087, -0.2995, -2.6556]],
                    [[2.0047, 2.8903, -0.2012, -0.3460, -2.6416], [2.0047, 2.8903, -0.2012, -0.3460, -2.6416]],
                    [[2.0194, 2.8948, -0.1911, -0.3908, -2.6262], [2.0194, 2.8948, -0.1911, -0.3908, -2.6262]],
                    [[2.0333, 2.8988, -0.1820, -0.4331, -2.6110], [2.0333, 2.8988, -0.1820, -0.4331, -2.6110]],
                    [[2.0466, 2.9024, -0.1737, -0.4733, -2.5961], [2.0466, 2.9024, -0.1737, -0.4733, -2.5961]],
                    [[2.0593, 2.9057, -0.1662, -0.5112, -2.5814], [2.0593, 2.9057, -0.1662, -0.5112, -2.5814]],
                    [[2.0713, 2.9085, -0.1595, -0.5471, -2.5671], [2.0713, 2.9085, -0.1595, -0.5471, -2.5671]],
                    [[2.0827, 2.9111, -0.1535, -0.5811, -2.5532], [2.0827, 2.9111, -0.1535, -0.5811, -2.5532]],
                    [[2.0936, 2.9135, -0.1482, -0.6131, -2.5396], [2.0936, 2.9135, -0.1482, -0.6131, -2.5396]],
                    [[2.1040, 2.9156, -0.1436, -0.6435, -2.5264], [2.1040, 2.9156, -0.1436, -0.6435, -2.5264]],
                    [[2.1138, 2.9175, -0.1396, -0.6721, -2.5136], [2.1138, 2.9175, -0.1396, -0.6721, -2.5136]],
                    [[2.1232, 2.9193, -0.1361, -0.6992, -2.5012], [2.1232, 2.9193, -0.1361, -0.6992, -2.5012]],
                    [[2.1322, 2.9209, -0.1331, -0.7247, -2.4892], [2.1322, 2.9209, -0.1331, -0.7247, -2.4892]],
                    [[2.1407, 2.9225, -0.1306, -0.7489, -2.4776], [2.1407, 2.9225, -0.1306, -0.7489, -2.4776]],
                    [[2.1488, 2.9239, -0.1285, -0.7717, -2.4665], [2.1488, 2.9239, -0.1285, -0.7717, -2.4665]],
                    [[2.1566, 2.9252, -0.1267, -0.7933, -2.4557], [2.1566, 2.9252, -0.1267, -0.7933, -2.4557]],
                    [[2.1639, 2.9265, -0.1252, -0.8136, -2.4455], [2.1639, 2.9265, -0.1252, -0.8136, -2.4455]],
                    [[2.1709, 2.9278, -0.1240, -0.8329, -2.4357], [2.1709, 2.9278, -0.1240, -0.8329, -2.4357]],
                    [[2.1775, 2.9291, -0.1231, -0.8511, -2.4263], [2.1775, 2.9291, -0.1231, -0.8511, -2.4263]],
                    [[2.1837, 2.9303, -0.1223, -0.8682, -2.4174], [2.1837, 2.9303, -0.1223, -0.8682, -2.4174]],
                    [[2.1896, 2.9315, -0.1217, -0.8844, -2.4089], [2.1896, 2.9315, -0.1217, -0.8844, -2.4089]],
                    [[2.1951, 2.9328, -0.1212, -0.8997, -2.4008], [2.1951, 2.9328, -0.1212, -0.8997, -2.4008]],
                    [[2.2002, 2.9341, -0.1208, -0.9142, -2.3932], [2.2002, 2.9341, -0.1208, -0.9142, -2.3932]],
                    [[2.2050, 2.9355, -0.1205, -0.9278, -2.3861], [2.2050, 2.9355, -0.1205, -0.9278, -2.3861]],
                    [[2.2094, 2.9369, -0.1202, -0.9407, -2.3794], [2.2094, 2.9369, -0.1202, -0.9407, -2.3794]],
                    [[2.2134, 2.9384, -0.1200, -0.9528, -2.3731], [2.2134, 2.9384, -0.1200, -0.9528, -2.3731]],
                    [[2.2171, 2.9400, -0.1197, -0.9642, -2.3672], [2.2171, 2.9400, -0.1197, -0.9642, -2.3672]],
                    [[2.2204, 2.9417, -0.1194, -0.9749, -2.3617], [2.2204, 2.9417, -0.1194, -0.9749, -2.3617]],
                    [[2.2233, 2.9435, -0.1191, -0.9851, -2.3566], [2.2233, 2.9435, -0.1191, -0.9851, -2.3566]],
                    [[2.2258, 2.9454, -0.1187, -0.9946, -2.3519], [2.2258, 2.9454, -0.1187, -0.9946, -2.3519]],
                    [[2.2280, 2.9475, -0.1182, -1.0035, -2.3476], [2.2280, 2.9475, -0.1182, -1.0035, -2.3476]],
                    [[2.2297, 2.9497, -0.1176, -1.0120, -2.3437], [2.2297, 2.9497, -0.1176, -1.0120, -2.3437]],
                    [[2.2310, 2.9520, -0.1170, -1.0199, -2.3401], [2.2310, 2.9520, -0.1170, -1.0199, -2.3401]],
                    [[2.2320, 2.9545, -0.1162, -1.0273, -2.3368], [2.2320, 2.9545, -0.1162, -1.0273, -2.3368]],
                    [[2.2325, 2.9571, -0.1153, -1.0343, -2.3338], [2.2325, 2.9571, -0.1153, -1.0343, -2.3338]],
                ]
            ),
            torch.tensor([[[0.9160, 3.1813, -1.0949, -0.2333, -1.0630], [0.9160, 3.1813, -1.0949, -0.2333, -1.0630]]]),
        ]

        # cls.enc_output_forprediction_hidden_states = [torch.rand(
        #     cls.params["context_length"],
        #     batch_size,
        #     cls.params["encoder_state_dim"],
        # ) for _ in range(cls.params["encoder_num_layers"] - 1)] + \
        # [torch.rand(
        #     cls.params["decoder_patch_len"] + 1,
        #     batch_size,
        #     cls.params["encoder_state_dim"],
        # )]

        cls.enc_output_forprediction_hidden_states = [
            torch.tensor(
                [
                    [[1.3950, 1.2677, 0.3756, 0.0789, -1.4111], [1.3950, 1.2677, 0.3756, 0.0789, -1.4111]],
                    [[1.4152, 1.2659, 0.3814, 0.0466, -1.4031], [1.4152, 1.2659, 0.3814, 0.0466, -1.4031]],
                    [[1.4319, 1.2655, 0.3874, 0.0164, -1.3951], [1.4319, 1.2655, 0.3874, 0.0164, -1.3951]],
                    [[1.4405, 1.2690, 0.3947, -0.0102, -1.3879], [1.4405, 1.2690, 0.3947, -0.0102, -1.3879]],
                    [[1.4488, 1.2722, 0.4015, -0.0357, -1.3806], [1.4488, 1.2722, 0.4015, -0.0357, -1.3806]],
                    [[1.4568, 1.2752, 0.4077, -0.0603, -1.3732], [1.4568, 1.2752, 0.4077, -0.0603, -1.3732]],
                    [[1.4644, 1.2779, 0.4133, -0.0838, -1.3657], [1.4644, 1.2779, 0.4133, -0.0838, -1.3657]],
                    [[1.4716, 1.2805, 0.4185, -0.1063, -1.3582], [1.4716, 1.2805, 0.4185, -0.1063, -1.3582]],
                    [[1.4785, 1.2830, 0.4231, -0.1278, -1.3507], [1.4785, 1.2830, 0.4231, -0.1278, -1.3507]],
                    [[1.4850, 1.2853, 0.4273, -0.1484, -1.3432], [1.4850, 1.2853, 0.4273, -0.1484, -1.3432]],
                    [[1.4913, 1.2875, 0.4310, -0.1679, -1.3358], [1.4913, 1.2875, 0.4310, -0.1679, -1.3358]],
                    [[1.4971, 1.2897, 0.4342, -0.1866, -1.3284], [1.4971, 1.2897, 0.4342, -0.1866, -1.3284]],
                    [[1.5026, 1.2918, 0.4371, -0.2044, -1.3210], [1.5026, 1.2918, 0.4371, -0.2044, -1.3210]],
                    [[1.5078, 1.2939, 0.4395, -0.2213, -1.3138], [1.5078, 1.2939, 0.4395, -0.2213, -1.3138]],
                    [[1.5127, 1.2959, 0.4415, -0.2373, -1.3067], [1.5127, 1.2959, 0.4415, -0.2373, -1.3067]],
                    [[1.5173, 1.2980, 0.4432, -0.2526, -1.2997], [1.5173, 1.2980, 0.4432, -0.2526, -1.2997]],
                    [[1.5215, 1.3001, 0.4445, -0.2671, -1.2929], [1.5215, 1.3001, 0.4445, -0.2671, -1.2929]],
                    [[1.5255, 1.3021, 0.4454, -0.2808, -1.2861], [1.5255, 1.3021, 0.4454, -0.2808, -1.2861]],
                    [[1.5291, 1.3043, 0.4461, -0.2939, -1.2795], [1.5291, 1.3043, 0.4461, -0.2939, -1.2795]],
                    [[1.5325, 1.3065, 0.4464, -0.3062, -1.2730], [1.5325, 1.3065, 0.4464, -0.3062, -1.2730]],
                    [[1.5355, 1.3087, 0.4465, -0.3180, -1.2667], [1.5355, 1.3087, 0.4465, -0.3180, -1.2667]],
                    [[1.5383, 1.3110, 0.4463, -0.3291, -1.2605], [1.5383, 1.3110, 0.4463, -0.3291, -1.2605]],
                    [[1.5408, 1.3134, 0.4458, -0.3396, -1.2545], [1.5408, 1.3134, 0.4458, -0.3396, -1.2545]],
                    [[1.5431, 1.3159, 0.4452, -0.3495, -1.2485], [1.5431, 1.3159, 0.4452, -0.3495, -1.2485]],
                    [[1.5450, 1.3185, 0.4442, -0.3589, -1.2428], [1.5450, 1.3185, 0.4442, -0.3589, -1.2428]],
                    [[1.5467, 1.3212, 0.4431, -0.3678, -1.2371], [1.5467, 1.3212, 0.4431, -0.3678, -1.2371]],
                    [[1.5482, 1.3240, 0.4418, -0.3762, -1.2316], [1.5482, 1.3240, 0.4418, -0.3762, -1.2316]],
                    [[1.5493, 1.3269, 0.4403, -0.3842, -1.2262], [1.5493, 1.3269, 0.4403, -0.3842, -1.2262]],
                    [[1.5503, 1.3299, 0.4386, -0.3917, -1.2209], [1.5503, 1.3299, 0.4386, -0.3917, -1.2209]],
                    [[1.5856, 1.9463, 0.3495, -1.2532, 0.3535], [1.5856, 1.9463, 0.3495, -1.2532, 0.3535]],
                    [[1.5876, 1.9456, 0.3452, -1.2520, 0.3552], [1.5876, 1.9456, 0.3452, -1.2520, 0.3552]],
                    [[1.5891, 1.9450, 0.3410, -1.2506, 0.3573], [1.5891, 1.9450, 0.3410, -1.2506, 0.3573]],
                    [[1.5900, 1.9445, 0.3368, -1.2493, 0.3597], [1.5900, 1.9445, 0.3368, -1.2493, 0.3597]],
                    [[1.5904, 1.9441, 0.3327, -1.2479, 0.3625], [1.5904, 1.9441, 0.3327, -1.2479, 0.3625]],
                    [[1.5903, 1.9437, 0.3288, -1.2465, 0.3655], [1.5903, 1.9437, 0.3288, -1.2465, 0.3655]],
                    [[1.5897, 1.9434, 0.3250, -1.2451, 0.3687], [1.5897, 1.9434, 0.3250, -1.2451, 0.3687]],
                ]
            ),
            torch.tensor(
                [
                    [[1.9439, 2.8983, -0.2151, -0.2526, -2.6684], [1.9439, 2.8983, -0.2151, -0.2526, -2.6684]],
                    [[1.9769, 2.8930, -0.2087, -0.2995, -2.6556], [1.9769, 2.8930, -0.2087, -0.2995, -2.6556]],
                    [[2.0047, 2.8903, -0.2012, -0.3460, -2.6416], [2.0047, 2.8903, -0.2012, -0.3460, -2.6416]],
                    [[2.0194, 2.8948, -0.1911, -0.3908, -2.6262], [2.0194, 2.8948, -0.1911, -0.3908, -2.6262]],
                    [[2.0333, 2.8988, -0.1820, -0.4331, -2.6110], [2.0333, 2.8988, -0.1820, -0.4331, -2.6110]],
                    [[2.0466, 2.9024, -0.1737, -0.4733, -2.5961], [2.0466, 2.9024, -0.1737, -0.4733, -2.5961]],
                    [[2.0593, 2.9057, -0.1662, -0.5112, -2.5814], [2.0593, 2.9057, -0.1662, -0.5112, -2.5814]],
                    [[2.0713, 2.9085, -0.1595, -0.5471, -2.5671], [2.0713, 2.9085, -0.1595, -0.5471, -2.5671]],
                    [[2.0827, 2.9111, -0.1535, -0.5811, -2.5532], [2.0827, 2.9111, -0.1535, -0.5811, -2.5532]],
                    [[2.0936, 2.9135, -0.1482, -0.6131, -2.5396], [2.0936, 2.9135, -0.1482, -0.6131, -2.5396]],
                    [[2.1040, 2.9156, -0.1436, -0.6435, -2.5264], [2.1040, 2.9156, -0.1436, -0.6435, -2.5264]],
                    [[2.1138, 2.9175, -0.1396, -0.6721, -2.5136], [2.1138, 2.9175, -0.1396, -0.6721, -2.5136]],
                    [[2.1232, 2.9193, -0.1361, -0.6992, -2.5012], [2.1232, 2.9193, -0.1361, -0.6992, -2.5012]],
                    [[2.1322, 2.9209, -0.1331, -0.7247, -2.4892], [2.1322, 2.9209, -0.1331, -0.7247, -2.4892]],
                    [[2.1407, 2.9225, -0.1306, -0.7489, -2.4776], [2.1407, 2.9225, -0.1306, -0.7489, -2.4776]],
                    [[2.1488, 2.9239, -0.1285, -0.7717, -2.4665], [2.1488, 2.9239, -0.1285, -0.7717, -2.4665]],
                    [[2.1566, 2.9252, -0.1267, -0.7933, -2.4557], [2.1566, 2.9252, -0.1267, -0.7933, -2.4557]],
                    [[2.1639, 2.9265, -0.1252, -0.8136, -2.4455], [2.1639, 2.9265, -0.1252, -0.8136, -2.4455]],
                    [[2.1709, 2.9278, -0.1240, -0.8329, -2.4357], [2.1709, 2.9278, -0.1240, -0.8329, -2.4357]],
                    [[2.1775, 2.9291, -0.1231, -0.8510, -2.4263], [2.1775, 2.9291, -0.1231, -0.8510, -2.4263]],
                    [[2.1837, 2.9303, -0.1223, -0.8682, -2.4174], [2.1837, 2.9303, -0.1223, -0.8682, -2.4174]],
                    [[2.1896, 2.9315, -0.1217, -0.8844, -2.4089], [2.1896, 2.9315, -0.1217, -0.8844, -2.4089]],
                    [[2.1951, 2.9328, -0.1212, -0.8997, -2.4008], [2.1951, 2.9328, -0.1212, -0.8997, -2.4008]],
                    [[2.2002, 2.9341, -0.1208, -0.9142, -2.3932], [2.2002, 2.9341, -0.1208, -0.9142, -2.3932]],
                    [[2.2050, 2.9355, -0.1205, -0.9278, -2.3861], [2.2050, 2.9355, -0.1205, -0.9278, -2.3861]],
                    [[2.2094, 2.9369, -0.1202, -0.9407, -2.3794], [2.2094, 2.9369, -0.1202, -0.9407, -2.3794]],
                    [[2.2134, 2.9384, -0.1200, -0.9528, -2.3731], [2.2134, 2.9384, -0.1200, -0.9528, -2.3731]],
                    [[2.2171, 2.9400, -0.1197, -0.9642, -2.3672], [2.2171, 2.9400, -0.1197, -0.9642, -2.3672]],
                    [[2.2204, 2.9417, -0.1194, -0.9749, -2.3617], [2.2204, 2.9417, -0.1194, -0.9749, -2.3617]],
                    [[2.4992, 3.3195, -0.3906, -2.5442, 0.0979], [2.4992, 3.3195, -0.3906, -2.5442, 0.0979]],
                    [[2.5058, 3.3152, -0.3979, -2.5420, 0.1007], [2.5058, 3.3152, -0.3979, -2.5420, 0.1007]],
                    [[2.5118, 3.3107, -0.4048, -2.5403, 0.1043], [2.5118, 3.3107, -0.4048, -2.5403, 0.1043]],
                    [[2.5174, 3.3060, -0.4112, -2.5391, 0.1087], [2.5174, 3.3060, -0.4112, -2.5391, 0.1087]],
                    [[2.5223, 3.3011, -0.4171, -2.5385, 0.1139], [2.5223, 3.3011, -0.4171, -2.5385, 0.1139]],
                    [[2.5267, 3.2962, -0.4225, -2.5384, 0.1198], [2.5267, 3.2962, -0.4225, -2.5384, 0.1198]],
                    [[2.5304, 3.2911, -0.4273, -2.5389, 0.1264], [2.5304, 3.2911, -0.4273, -2.5389, 0.1264]],
                ]
            ),
            torch.tensor(
                [
                    [[0.9559, 3.1517, -1.1341, -0.2454, -1.0221], [0.9559, 3.1517, -1.1341, -0.2454, -1.0221]],
                    [[1.7673, 3.5491, -1.5643, -0.8036, 0.0332], [1.7673, 3.5491, -1.5643, -0.8036, 0.0332]],
                    [[1.7825, 3.5710, -1.6063, -0.8246, 0.0592], [1.7825, 3.5710, -1.6063, -0.8246, 0.0592]],
                    [[1.7993, 3.5916, -1.6465, -0.8459, 0.0832], [1.7993, 3.5916, -1.6465, -0.8459, 0.0832]],
                    [[1.8173, 3.6104, -1.6838, -0.8666, 0.1045], [1.8173, 3.6104, -1.6838, -0.8666, 0.1045]],
                    [[1.8359, 3.6270, -1.7176, -0.8859, 0.1224], [1.8359, 3.6270, -1.7176, -0.8859, 0.1224]],
                    [[1.8547, 3.6412, -1.7474, -0.9033, 0.1366], [1.8547, 3.6412, -1.7474, -0.9033, 0.1366]],
                    [[1.8731, 3.6527, -1.7728, -0.9183, 0.1470], [1.8731, 3.6527, -1.7728, -0.9183, 0.1470]],
                ]
            ),
        ]

        # cls.enc_output_last_hidden_state = torch.rand(
        #     1,
        #     batch_size,
        #     cls.params["encoder_state_dim"],
        # )
        cls.enc_output_last_hidden_state = torch.tensor(
            [[[0.9160, 3.1813, -1.0949, -0.2333, -1.0630], [0.9160, 3.1813, -1.0949, -0.2333, -1.0630]]]
        )

        # cls.enc_output_forprediction_last_hidden_state = torch.rand(
        #     cls.params["decoder_patch_len"] + 1,
        #     batch_size,
        #     cls.params["encoder_state_dim"],
        # )
        cls.enc_output_forprediction_last_hidden_state = torch.tensor(
            [
                [[0.9559, 3.1517, -1.1341, -0.2454, -1.0221], [0.9559, 3.1517, -1.1341, -0.2454, -1.0221]],
                [[1.7673, 3.5491, -1.5643, -0.8036, 0.0332], [1.7673, 3.5491, -1.5643, -0.8036, 0.0332]],
                [[1.7825, 3.5710, -1.6063, -0.8246, 0.0592], [1.7825, 3.5710, -1.6063, -0.8246, 0.0592]],
                [[1.7993, 3.5916, -1.6465, -0.8459, 0.0832], [1.7993, 3.5916, -1.6465, -0.8459, 0.0832]],
                [[1.8173, 3.6104, -1.6838, -0.8666, 0.1045], [1.8173, 3.6104, -1.6838, -0.8666, 0.1045]],
                [[1.8359, 3.6270, -1.7176, -0.8859, 0.1224], [1.8359, 3.6270, -1.7176, -0.8859, 0.1224]],
                [[1.8547, 3.6412, -1.7474, -0.9033, 0.1366], [1.8547, 3.6412, -1.7474, -0.9033, 0.1366]],
                [[1.8731, 3.6527, -1.7728, -0.9183, 0.1470], [1.8731, 3.6527, -1.7728, -0.9183, 0.1470]],
            ]
        )

        # cls.dec_output_last_hidden_state = torch.rand(
        #     1,
        #     batch_size,
        #     len(cls.params["quantiles"]) if cls.params["prediction_type"] == 'quantile' else 1,
        #     cls.params["decoder_patch_len"],
        #     cls.params["num_input_channels"],
        # )
        cls.dec_output_last_hidden_state = torch.tensor(
            [
                [
                    [
                        [[0.1641], [-0.1768], [-0.3237], [-0.4347], [-0.1106]],
                        [[-0.2644], [-0.1551], [-0.2287], [-0.5890], [-0.7668]],
                        [[0.3588], [0.3925], [0.3508], [-0.0250], [0.1464]],
                        [[0.4988], [0.2703], [0.2012], [0.2671], [-0.0760]],
                        [[0.1450], [0.4185], [0.9390], [0.7325], [0.8419]],
                        [[-0.5661], [-0.5043], [-0.3809], [-0.1840], [-0.3531]],
                        [[-0.3666], [-0.4315], [-0.1276], [0.3259], [0.2659]],
                        [[0.0355], [-0.4043], [-0.2793], [-0.1048], [-0.4744]],
                        [[-0.2119], [-0.1897], [0.3908], [0.0181], [-0.1842]],
                    ],
                    [
                        [[0.1641], [-0.1768], [-0.3237], [-0.4347], [-0.1106]],
                        [[-0.2644], [-0.1551], [-0.2287], [-0.5890], [-0.7668]],
                        [[0.3588], [0.3925], [0.3508], [-0.0250], [0.1464]],
                        [[0.4988], [0.2703], [0.2012], [0.2671], [-0.0760]],
                        [[0.1450], [0.4185], [0.9390], [0.7325], [0.8419]],
                        [[-0.5661], [-0.5043], [-0.3809], [-0.1840], [-0.3531]],
                        [[-0.3666], [-0.4315], [-0.1276], [0.3259], [0.2659]],
                        [[0.0355], [-0.4043], [-0.2793], [-0.1048], [-0.4744]],
                        [[-0.2119], [-0.1897], [0.3908], [0.0181], [-0.1842]],
                    ],
                ]
            ]
        )

        # cls.dec_output_forprediction_last_hidden_state = torch.rand(
        #     cls.params["decoder_patch_len"] + 1,
        #     batch_size,
        #     len(cls.params["quantiles"]) if cls.params["prediction_type"] == 'quantile' else 1,
        #     cls.params["decoder_patch_len"],
        #     cls.params["num_input_channels"],
        # )
        cls.dec_output_forprediction_last_hidden_state = torch.tensor(
            [
                [
                    [
                        [[0.1605], [-0.1805], [-0.3208], [-0.4351], [-0.1164]],
                        [[-0.2681], [-0.1596], [-0.2362], [-0.5895], [-0.7693]],
                        [[0.3555], [0.3905], [0.3451], [-0.0341], [0.1477]],
                        [[0.5078], [0.2724], [0.1989], [0.2631], [-0.0775]],
                        [[0.1475], [0.4234], [0.9459], [0.7381], [0.8453]],
                        [[-0.5603], [-0.4921], [-0.3695], [-0.1744], [-0.3454]],
                        [[-0.3631], [-0.4323], [-0.1267], [0.3305], [0.2555]],
                        [[0.0425], [-0.3894], [-0.2718], [-0.1072], [-0.4696]],
                        [[-0.2120], [-0.1855], [0.3963], [0.0192], [-0.1930]],
                    ],
                    [
                        [[0.1605], [-0.1805], [-0.3208], [-0.4351], [-0.1164]],
                        [[-0.2681], [-0.1596], [-0.2362], [-0.5895], [-0.7693]],
                        [[0.3555], [0.3905], [0.3451], [-0.0341], [0.1477]],
                        [[0.5078], [0.2724], [0.1989], [0.2631], [-0.0775]],
                        [[0.1475], [0.4234], [0.9459], [0.7381], [0.8453]],
                        [[-0.5603], [-0.4921], [-0.3695], [-0.1744], [-0.3454]],
                        [[-0.3631], [-0.4323], [-0.1267], [0.3305], [0.2555]],
                        [[0.0425], [-0.3894], [-0.2718], [-0.1072], [-0.4696]],
                        [[-0.2120], [-0.1855], [0.3963], [0.0192], [-0.1930]],
                    ],
                ],
                [
                    [
                        [[0.1506], [-0.2933], [-0.2759], [-0.5320], [-0.2430]],
                        [[-0.3405], [-0.2354], [-0.3918], [-0.7291], [-0.9835]],
                        [[0.2977], [0.4314], [0.3221], [-0.1201], [0.2824]],
                        [[0.8198], [0.3860], [0.2984], [0.3738], [-0.0547]],
                        [[0.2310], [0.5455], [1.2039], [0.8282], [1.0119]],
                        [[-0.4952], [-0.3891], [-0.2671], [-0.0320], [-0.3600]],
                        [[-0.2599], [-0.3800], [-0.0540], [0.5179], [0.1024]],
                        [[0.1273], [-0.2598], [-0.1807], [-0.1802], [-0.4349]],
                        [[-0.2936], [-0.1226], [0.5532], [0.0026], [-0.4380]],
                    ],
                    [
                        [[0.1506], [-0.2933], [-0.2759], [-0.5320], [-0.2430]],
                        [[-0.3405], [-0.2354], [-0.3918], [-0.7291], [-0.9835]],
                        [[0.2977], [0.4314], [0.3221], [-0.1201], [0.2824]],
                        [[0.8198], [0.3860], [0.2984], [0.3738], [-0.0547]],
                        [[0.2310], [0.5455], [1.2039], [0.8282], [1.0119]],
                        [[-0.4952], [-0.3891], [-0.2671], [-0.0320], [-0.3600]],
                        [[-0.2599], [-0.3800], [-0.0540], [0.5179], [0.1024]],
                        [[0.1273], [-0.2598], [-0.1807], [-0.1802], [-0.4349]],
                        [[-0.2936], [-0.1226], [0.5532], [0.0026], [-0.4380]],
                    ],
                ],
                [
                    [
                        [[0.1507], [-0.2991], [-0.2781], [-0.5358], [-0.2495]],
                        [[-0.3445], [-0.2428], [-0.4001], [-0.7353], [-0.9972]],
                        [[0.2987], [0.4336], [0.3222], [-0.1259], [0.2853]],
                        [[0.8339], [0.3906], [0.3021], [0.3767], [-0.0501]],
                        [[0.2342], [0.5537], [1.2179], [0.8363], [1.0193]],
                        [[-0.4952], [-0.3891], [-0.2662], [-0.0291], [-0.3601]],
                        [[-0.2600], [-0.3834], [-0.0531], [0.5256], [0.0997]],
                        [[0.1340], [-0.2550], [-0.1808], [-0.1828], [-0.4372]],
                        [[-0.3001], [-0.1240], [0.5583], [0.0021], [-0.4464]],
                    ],
                    [
                        [[0.1507], [-0.2991], [-0.2781], [-0.5358], [-0.2495]],
                        [[-0.3445], [-0.2428], [-0.4001], [-0.7353], [-0.9972]],
                        [[0.2987], [0.4336], [0.3222], [-0.1259], [0.2853]],
                        [[0.8339], [0.3906], [0.3021], [0.3767], [-0.0501]],
                        [[0.2342], [0.5537], [1.2179], [0.8363], [1.0193]],
                        [[-0.4952], [-0.3891], [-0.2662], [-0.0291], [-0.3601]],
                        [[-0.2600], [-0.3834], [-0.0531], [0.5256], [0.0997]],
                        [[0.1340], [-0.2550], [-0.1808], [-0.1828], [-0.4372]],
                        [[-0.3001], [-0.1240], [0.5583], [0.0021], [-0.4464]],
                    ],
                ],
                [
                    [
                        [[0.1507], [-0.3046], [-0.2801], [-0.5393], [-0.2554]],
                        [[-0.3485], [-0.2500], [-0.4083], [-0.7414], [-1.0106]],
                        [[0.2997], [0.4357], [0.3223], [-0.1314], [0.2879]],
                        [[0.8474], [0.3951], [0.3056], [0.3795], [-0.0458]],
                        [[0.2374], [0.5616], [1.2315], [0.8444], [1.0268]],
                        [[-0.4950], [-0.3887], [-0.2649], [-0.0261], [-0.3599]],
                        [[-0.2601], [-0.3865], [-0.0521], [0.5331], [0.0971]],
                        [[0.1402], [-0.2504], [-0.1807], [-0.1852], [-0.4392]],
                        [[-0.3062], [-0.1249], [0.5635], [0.0017], [-0.4547]],
                    ],
                    [
                        [[0.1507], [-0.3046], [-0.2801], [-0.5393], [-0.2554]],
                        [[-0.3485], [-0.2500], [-0.4083], [-0.7414], [-1.0106]],
                        [[0.2997], [0.4357], [0.3223], [-0.1314], [0.2879]],
                        [[0.8474], [0.3951], [0.3056], [0.3795], [-0.0458]],
                        [[0.2374], [0.5616], [1.2315], [0.8444], [1.0268]],
                        [[-0.4950], [-0.3887], [-0.2649], [-0.0261], [-0.3599]],
                        [[-0.2601], [-0.3865], [-0.0521], [0.5331], [0.0971]],
                        [[0.1402], [-0.2504], [-0.1807], [-0.1852], [-0.4392]],
                        [[-0.3062], [-0.1249], [0.5635], [0.0017], [-0.4547]],
                    ],
                ],
                [
                    [
                        [[0.1506], [-0.3097], [-0.2818], [-0.5426], [-0.2606]],
                        [[-0.3525], [-0.2567], [-0.4162], [-0.7475], [-1.0233]],
                        [[0.3008], [0.4378], [0.3225], [-0.1365], [0.2901]],
                        [[0.8597], [0.3994], [0.3087], [0.3819], [-0.0418]],
                        [[0.2405], [0.5691], [1.2446], [0.8525], [1.0342]],
                        [[-0.4945], [-0.3878], [-0.2632], [-0.0230], [-0.3593]],
                        [[-0.2602], [-0.3893], [-0.0512], [0.5404], [0.0949]],
                        [[0.1457], [-0.2462], [-0.1804], [-0.1874], [-0.4410]],
                        [[-0.3117], [-0.1255], [0.5688], [0.0015], [-0.4627]],
                    ],
                    [
                        [[0.1506], [-0.3097], [-0.2818], [-0.5426], [-0.2606]],
                        [[-0.3525], [-0.2567], [-0.4162], [-0.7475], [-1.0233]],
                        [[0.3008], [0.4378], [0.3225], [-0.1365], [0.2901]],
                        [[0.8597], [0.3994], [0.3087], [0.3819], [-0.0418]],
                        [[0.2405], [0.5691], [1.2446], [0.8525], [1.0342]],
                        [[-0.4945], [-0.3878], [-0.2632], [-0.0230], [-0.3593]],
                        [[-0.2602], [-0.3893], [-0.0512], [0.5404], [0.0949]],
                        [[0.1457], [-0.2462], [-0.1804], [-0.1874], [-0.4410]],
                        [[-0.3117], [-0.1255], [0.5688], [0.0015], [-0.4627]],
                    ],
                ],
                [
                    [
                        [[0.1504], [-0.3143], [-0.2833], [-0.5456], [-0.2651]],
                        [[-0.3563], [-0.2628], [-0.4236], [-0.7533], [-1.0350]],
                        [[0.3018], [0.4397], [0.3227], [-0.1412], [0.2920]],
                        [[0.8706], [0.4035], [0.3115], [0.3839], [-0.0384]],
                        [[0.2435], [0.5759], [1.2566], [0.8603], [1.0413]],
                        [[-0.4939], [-0.3865], [-0.2612], [-0.0200], [-0.3583]],
                        [[-0.2603], [-0.3918], [-0.0503], [0.5471], [0.0930]],
                        [[0.1504], [-0.2424], [-0.1798], [-0.1893], [-0.4424]],
                        [[-0.3164], [-0.1257], [0.5740], [0.0014], [-0.4701]],
                    ],
                    [
                        [[0.1504], [-0.3143], [-0.2833], [-0.5456], [-0.2651]],
                        [[-0.3563], [-0.2628], [-0.4236], [-0.7533], [-1.0350]],
                        [[0.3018], [0.4397], [0.3227], [-0.1412], [0.2920]],
                        [[0.8706], [0.4035], [0.3115], [0.3839], [-0.0384]],
                        [[0.2435], [0.5759], [1.2566], [0.8603], [1.0413]],
                        [[-0.4939], [-0.3865], [-0.2612], [-0.0200], [-0.3583]],
                        [[-0.2603], [-0.3918], [-0.0503], [0.5471], [0.0930]],
                        [[0.1504], [-0.2424], [-0.1798], [-0.1893], [-0.4424]],
                        [[-0.3164], [-0.1257], [0.5740], [0.0014], [-0.4701]],
                    ],
                ],
                [
                    [
                        [[0.1500], [-0.3184], [-0.2847], [-0.5483], [-0.2688]],
                        [[-0.3599], [-0.2683], [-0.4304], [-0.7588], [-1.0454]],
                        [[0.3030], [0.4414], [0.3230], [-0.1453], [0.2934]],
                        [[0.8801], [0.4072], [0.3137], [0.3855], [-0.0355]],
                        [[0.2462], [0.5820], [1.2675], [0.8677], [1.0479]],
                        [[-0.4932], [-0.3850], [-0.2591], [-0.0171], [-0.3571]],
                        [[-0.2606], [-0.3941], [-0.0495], [0.5532], [0.0914]],
                        [[0.1543], [-0.2391], [-0.1792], [-0.1908], [-0.4436]],
                        [[-0.3203], [-0.1256], [0.5788], [0.0015], [-0.4769]],
                    ],
                    [
                        [[0.1500], [-0.3184], [-0.2847], [-0.5483], [-0.2688]],
                        [[-0.3599], [-0.2683], [-0.4304], [-0.7588], [-1.0454]],
                        [[0.3030], [0.4414], [0.3230], [-0.1453], [0.2934]],
                        [[0.8801], [0.4072], [0.3137], [0.3855], [-0.0355]],
                        [[0.2462], [0.5820], [1.2675], [0.8677], [1.0479]],
                        [[-0.4932], [-0.3850], [-0.2591], [-0.0171], [-0.3571]],
                        [[-0.2606], [-0.3941], [-0.0495], [0.5532], [0.0914]],
                        [[0.1543], [-0.2391], [-0.1792], [-0.1908], [-0.4436]],
                        [[-0.3203], [-0.1256], [0.5788], [0.0015], [-0.4769]],
                    ],
                ],
                [
                    [
                        [[0.1496], [-0.3218], [-0.2858], [-0.5507], [-0.2718]],
                        [[-0.3632], [-0.2730], [-0.4364], [-0.7637], [-1.0543]],
                        [[0.3041], [0.4428], [0.3233], [-0.1489], [0.2943]],
                        [[0.8878], [0.4105], [0.3155], [0.3867], [-0.0334]],
                        [[0.2486], [0.5873], [1.2769], [0.8744], [1.0540]],
                        [[-0.4925], [-0.3833], [-0.2568], [-0.0144], [-0.3557]],
                        [[-0.2609], [-0.3961], [-0.0489], [0.5585], [0.0902]],
                        [[0.1574], [-0.2363], [-0.1783], [-0.1920], [-0.4444]],
                        [[-0.3233], [-0.1252], [0.5833], [0.0016], [-0.4829]],
                    ],
                    [
                        [[0.1496], [-0.3218], [-0.2858], [-0.5507], [-0.2718]],
                        [[-0.3632], [-0.2730], [-0.4364], [-0.7637], [-1.0543]],
                        [[0.3041], [0.4428], [0.3233], [-0.1489], [0.2943]],
                        [[0.8878], [0.4105], [0.3155], [0.3867], [-0.0334]],
                        [[0.2486], [0.5873], [1.2769], [0.8744], [1.0540]],
                        [[-0.4925], [-0.3833], [-0.2568], [-0.0144], [-0.3557]],
                        [[-0.2609], [-0.3961], [-0.0489], [0.5585], [0.0902]],
                        [[0.1574], [-0.2363], [-0.1783], [-0.1920], [-0.4444]],
                        [[-0.3233], [-0.1252], [0.5833], [0.0016], [-0.4829]],
                    ],
                ],
            ]
        )

        # cls.dec_output_hidden_states = [torch.rand(
        #     1,
        #     batch_size,
        #     len(cls.params["quantiles"]) if cls.params["prediction_type"] == 'quantile' else 1,
        #     cls.params["decoder_dim"]
        # ) for _ in range(1)]
        cls.dec_output_hidden_states = [
            torch.tensor(
                [
                    [
                        [
                            [-0.9459, -0.5805, -0.2807, 1.8970, -0.9403, 1.2147],
                            [-1.3169, -1.5278, -0.6026, -0.4899, 1.0868, -0.0934],
                            [1.2778, -1.7415, 0.5102, -0.6563, 1.7421, 0.0885],
                            [0.9334, -0.3303, -0.7916, 0.5959, -1.5051, 0.4242],
                            [2.1983, 1.4427, -0.7018, -1.1331, 1.2979, 1.3223],
                            [-1.4186, 0.3909, 0.8254, -1.3253, 0.2048, -0.6199],
                            [-0.4614, 2.0001, 0.7812, -0.7752, -0.9878, 0.1676],
                            [-0.6510, -0.9826, 1.0114, -1.1471, -1.0339, 0.5914],
                            [-0.1376, 0.0026, -0.8831, -1.7181, 1.0607, 2.1431],
                        ],
                        [
                            [-0.9459, -0.5805, -0.2807, 1.8970, -0.9403, 1.2147],
                            [-1.3169, -1.5278, -0.6026, -0.4899, 1.0868, -0.0934],
                            [1.2778, -1.7415, 0.5102, -0.6563, 1.7421, 0.0885],
                            [0.9334, -0.3303, -0.7916, 0.5959, -1.5051, 0.4242],
                            [2.1983, 1.4427, -0.7018, -1.1331, 1.2979, 1.3223],
                            [-1.4186, 0.3909, 0.8254, -1.3253, 0.2048, -0.6199],
                            [-0.4614, 2.0001, 0.7812, -0.7752, -0.9878, 0.1676],
                            [-0.6510, -0.9826, 1.0114, -1.1471, -1.0339, 0.5914],
                            [-0.1376, 0.0026, -0.8831, -1.7181, 1.0607, 2.1431],
                        ],
                    ]
                ]
            )
        ]

        # cls.dec_output_forprediction_hidden_states = [torch.rand(
        #     cls.params["decoder_patch_len"] + 1,
        #     batch_size,
        #     len(cls.params["quantiles"]) if cls.params["prediction_type"] == 'quantile' else 1,
        #     cls.params["decoder_dim"]
        # ) for _ in range(1)]
        cls.dec_output_forprediction_hidden_states = [
            torch.tensor(
                [
                    [
                        [
                            [-0.9527, -0.5823, -0.2907, 1.8720, -0.9327, 1.2355],
                            [-1.3325, -1.5142, -0.5943, -0.4829, 1.0647, -0.1124],
                            [1.2579, -1.7429, 0.5083, -0.6264, 1.7527, 0.0901],
                            [0.9393, -0.3629, -0.7794, 0.6107, -1.5131, 0.4312],
                            [2.2138, 1.4549, -0.7221, -1.1304, 1.2957, 1.3296],
                            [-1.3811, 0.3969, 0.8110, -1.3290, 0.2190, -0.6350],
                            [-0.4631, 2.0055, 0.7521, -0.7755, -1.0323, 0.1795],
                            [-0.6278, -0.9996, 0.9832, -1.1143, -1.0025, 0.5784],
                            [-0.1308, -0.0101, -0.9056, -1.7396, 1.0731, 2.1382],
                        ],
                        [
                            [-0.9527, -0.5823, -0.2907, 1.8720, -0.9327, 1.2355],
                            [-1.3325, -1.5142, -0.5943, -0.4829, 1.0647, -0.1124],
                            [1.2579, -1.7429, 0.5083, -0.6264, 1.7527, 0.0901],
                            [0.9393, -0.3629, -0.7794, 0.6107, -1.5131, 0.4312],
                            [2.2138, 1.4549, -0.7221, -1.1304, 1.2957, 1.3296],
                            [-1.3811, 0.3969, 0.8110, -1.3290, 0.2190, -0.6350],
                            [-0.4631, 2.0055, 0.7521, -0.7755, -1.0323, 0.1795],
                            [-0.6278, -0.9996, 0.9832, -1.1143, -1.0025, 0.5784],
                            [-0.1308, -0.0101, -0.9056, -1.7396, 1.0731, 2.1382],
                        ],
                    ],
                    [
                        [
                            [-1.2484, -0.7509, -0.7262, 1.7929, -0.9721, 2.3076],
                            [-1.8004, -1.7402, -0.6527, -0.3902, 0.9132, -0.3994],
                            [1.2392, -1.7805, 0.7570, -0.4383, 2.3904, -0.1979],
                            [1.5484, -0.9266, -0.6203, 0.6444, -2.0171, 0.7977],
                            [2.6919, 1.5487, -1.1766, -1.1680, 1.7264, 2.0700],
                            [-0.9621, 0.3347, 0.8172, -1.7376, 0.2087, -0.9681],
                            [-0.2133, 2.1793, 0.2671, -0.9332, -1.9022, 0.1316],
                            [-0.3998, -1.4429, 0.8259, -0.9486, -0.4525, 0.6923],
                            [-0.0392, -0.4884, -1.2972, -2.7215, 1.8791, 2.0733],
                        ],
                        [
                            [-1.2484, -0.7509, -0.7262, 1.7929, -0.9721, 2.3076],
                            [-1.8004, -1.7402, -0.6527, -0.3902, 0.9132, -0.3994],
                            [1.2392, -1.7805, 0.7570, -0.4383, 2.3904, -0.1979],
                            [1.5484, -0.9266, -0.6203, 0.6444, -2.0171, 0.7977],
                            [2.6919, 1.5487, -1.1766, -1.1680, 1.7264, 2.0700],
                            [-0.9621, 0.3347, 0.8172, -1.7376, 0.2087, -0.9681],
                            [-0.2133, 2.1793, 0.2671, -0.9332, -1.9022, 0.1316],
                            [-0.3998, -1.4429, 0.8259, -0.9486, -0.4525, 0.6923],
                            [-0.0392, -0.4884, -1.2972, -2.7215, 1.8791, 2.0733],
                        ],
                    ],
                    [
                        [
                            [-1.2632, -0.7594, -0.7319, 1.7912, -0.9859, 2.3365],
                            [-1.8278, -1.7522, -0.6525, -0.3977, 0.8959, -0.4027],
                            [1.2394, -1.7974, 0.7624, -0.4286, 2.4165, -0.1950],
                            [1.5739, -0.9519, -0.6083, 0.6551, -2.0343, 0.8205],
                            [2.7214, 1.5603, -1.2020, -1.1776, 1.7458, 2.0889],
                            [-0.9549, 0.3309, 0.8310, -1.7576, 0.2127, -0.9779],
                            [-0.2119, 2.1997, 0.2603, -0.9463, -1.9367, 0.1386],
                            [-0.3908, -1.4701, 0.8288, -0.9392, -0.4505, 0.6855],
                            [-0.0449, -0.4913, -1.3167, -2.7571, 1.9039, 2.0820],
                        ],
                        [
                            [-1.2632, -0.7594, -0.7319, 1.7912, -0.9859, 2.3365],
                            [-1.8278, -1.7522, -0.6525, -0.3977, 0.8959, -0.4027],
                            [1.2394, -1.7974, 0.7624, -0.4286, 2.4165, -0.1950],
                            [1.5739, -0.9519, -0.6083, 0.6551, -2.0343, 0.8205],
                            [2.7214, 1.5603, -1.2020, -1.1776, 1.7458, 2.0889],
                            [-0.9549, 0.3309, 0.8310, -1.7576, 0.2127, -0.9779],
                            [-0.2119, 2.1997, 0.2603, -0.9463, -1.9367, 0.1386],
                            [-0.3908, -1.4701, 0.8288, -0.9392, -0.4505, 0.6855],
                            [-0.0449, -0.4913, -1.3167, -2.7571, 1.9039, 2.0820],
                        ],
                    ],
                    [
                        [
                            [-1.2774, -0.7664, -0.7383, 1.7904, -0.9991, 2.3650],
                            [-1.8548, -1.7640, -0.6525, -0.4045, 0.8797, -0.4065],
                            [1.2398, -1.8143, 0.7680, -0.4200, 2.4418, -0.1923],
                            [1.5982, -0.9760, -0.5973, 0.6658, -2.0503, 0.8414],
                            [2.7504, 1.5725, -1.2265, -1.1870, 1.7637, 2.1075],
                            [-0.9468, 0.3275, 0.8431, -1.7762, 0.2171, -0.9870],
                            [-0.2103, 2.2202, 0.2530, -0.9582, -1.9705, 0.1449],
                            [-0.3821, -1.4955, 0.8312, -0.9310, -0.4474, 0.6794],
                            [-0.0497, -0.4944, -1.3369, -2.7914, 1.9278, 2.0905],
                        ],
                        [
                            [-1.2774, -0.7664, -0.7383, 1.7904, -0.9991, 2.3650],
                            [-1.8548, -1.7640, -0.6525, -0.4045, 0.8797, -0.4065],
                            [1.2398, -1.8143, 0.7680, -0.4200, 2.4418, -0.1923],
                            [1.5982, -0.9760, -0.5973, 0.6658, -2.0503, 0.8414],
                            [2.7504, 1.5725, -1.2265, -1.1870, 1.7637, 2.1075],
                            [-0.9468, 0.3275, 0.8431, -1.7762, 0.2171, -0.9870],
                            [-0.2103, 2.2202, 0.2530, -0.9582, -1.9705, 0.1449],
                            [-0.3821, -1.4955, 0.8312, -0.9310, -0.4474, 0.6794],
                            [-0.0497, -0.4944, -1.3369, -2.7914, 1.9278, 2.0905],
                        ],
                    ],
                    [
                        [
                            [-1.2907, -0.7718, -0.7451, 1.7906, -1.0113, 2.3922],
                            [-1.8809, -1.7752, -0.6527, -0.4104, 0.8650, -0.4108],
                            [1.2405, -1.8308, 0.7736, -0.4127, 2.4655, -0.1897],
                            [1.6204, -0.9983, -0.5876, 0.6764, -2.0647, 0.8597],
                            [2.7784, 1.5850, -1.2496, -1.1957, 1.7797, 2.1255],
                            [-0.9380, 0.3247, 0.8534, -1.7930, 0.2219, -0.9952],
                            [-0.2087, 2.2403, 0.2456, -0.9687, -2.0029, 0.1504],
                            [-0.3741, -1.5184, 0.8330, -0.9242, -0.4435, 0.6741],
                            [-0.0535, -0.4978, -1.3572, -2.8236, 1.9501, 2.0990],
                        ],
                        [
                            [-1.2907, -0.7718, -0.7451, 1.7906, -1.0113, 2.3922],
                            [-1.8809, -1.7752, -0.6527, -0.4104, 0.8650, -0.4108],
                            [1.2405, -1.8308, 0.7736, -0.4127, 2.4655, -0.1897],
                            [1.6204, -0.9983, -0.5876, 0.6764, -2.0647, 0.8597],
                            [2.7784, 1.5850, -1.2496, -1.1957, 1.7797, 2.1255],
                            [-0.9380, 0.3247, 0.8534, -1.7930, 0.2219, -0.9952],
                            [-0.2087, 2.2403, 0.2456, -0.9687, -2.0029, 0.1504],
                            [-0.3741, -1.5184, 0.8330, -0.9242, -0.4435, 0.6741],
                            [-0.0535, -0.4978, -1.3572, -2.8236, 1.9501, 2.0990],
                        ],
                    ],
                    [
                        [
                            [-1.3031, -0.7758, -0.7521, 1.7918, -1.0224, 2.4174],
                            [-1.9053, -1.7856, -0.6529, -0.4152, 0.8522, -0.4154],
                            [1.2414, -1.8467, 0.7788, -0.4069, 2.4871, -0.1872],
                            [1.6402, -1.0183, -0.5794, 0.6865, -2.0772, 0.8749],
                            [2.8046, 1.5977, -1.2706, -1.2036, 1.7934, 2.1422],
                            [-0.9289, 0.3227, 0.8617, -1.8077, 0.2269, -1.0025],
                            [-0.2073, 2.2596, 0.2383, -0.9777, -2.0329, 0.1552],
                            [-0.3667, -1.5384, 0.8342, -0.9189, -0.4389, 0.6697],
                            [-0.0562, -0.5012, -1.3773, -2.8527, 1.9703, 2.1071],
                        ],
                        [
                            [-1.3031, -0.7758, -0.7521, 1.7918, -1.0224, 2.4174],
                            [-1.9053, -1.7856, -0.6529, -0.4152, 0.8522, -0.4154],
                            [1.2414, -1.8467, 0.7788, -0.4069, 2.4871, -0.1872],
                            [1.6402, -1.0183, -0.5794, 0.6865, -2.0772, 0.8749],
                            [2.8046, 1.5977, -1.2706, -1.2036, 1.7934, 2.1422],
                            [-0.9289, 0.3227, 0.8617, -1.8077, 0.2269, -1.0025],
                            [-0.2073, 2.2596, 0.2383, -0.9777, -2.0329, 0.1552],
                            [-0.3667, -1.5384, 0.8342, -0.9189, -0.4389, 0.6697],
                            [-0.0562, -0.5012, -1.3773, -2.8527, 1.9703, 2.1071],
                        ],
                    ],
                    [
                        [
                            [-1.3142, -0.7782, -0.7591, 1.7939, -1.0320, 2.4398],
                            [-1.9275, -1.7949, -0.6531, -0.4189, 0.8415, -0.4203],
                            [1.2424, -1.8616, 0.7834, -0.4024, 2.5062, -0.1847],
                            [1.6571, -1.0357, -0.5729, 0.6960, -2.0878, 0.8869],
                            [2.8285, 1.6102, -1.2893, -1.2106, 1.8045, 2.1573],
                            [-0.9197, 0.3214, 0.8679, -1.8200, 0.2320, -1.0087],
                            [-0.2061, 2.2778, 0.2315, -0.9852, -2.0600, 0.1593],
                            [-0.3602, -1.5553, 0.8350, -0.9149, -0.4341, 0.6663],
                            [-0.0580, -0.5046, -1.3964, -2.8782, 1.9878, 2.1150],
                        ],
                        [
                            [-1.3142, -0.7782, -0.7591, 1.7939, -1.0320, 2.4398],
                            [-1.9275, -1.7949, -0.6531, -0.4189, 0.8415, -0.4203],
                            [1.2424, -1.8616, 0.7834, -0.4024, 2.5062, -0.1847],
                            [1.6571, -1.0357, -0.5729, 0.6960, -2.0878, 0.8869],
                            [2.8285, 1.6102, -1.2893, -1.2106, 1.8045, 2.1573],
                            [-0.9197, 0.3214, 0.8679, -1.8200, 0.2320, -1.0087],
                            [-0.2061, 2.2778, 0.2315, -0.9852, -2.0600, 0.1593],
                            [-0.3602, -1.5553, 0.8350, -0.9149, -0.4341, 0.6663],
                            [-0.0580, -0.5046, -1.3964, -2.8782, 1.9878, 2.1150],
                        ],
                    ],
                    [
                        [
                            [-1.3240, -0.7793, -0.7658, 1.7967, -1.0400, 2.4591],
                            [-1.9471, -1.8028, -0.6532, -0.4214, 0.8330, -0.4253],
                            [1.2435, -1.8752, 0.7873, -0.3992, 2.5225, -0.1823],
                            [1.6708, -1.0504, -0.5682, 0.7047, -2.0963, 0.8956],
                            [2.8497, 1.6224, -1.3052, -1.2166, 1.8128, 2.1704],
                            [-0.9109, 0.3209, 0.8719, -1.8299, 0.2371, -1.0139],
                            [-0.2054, 2.2945, 0.2253, -0.9909, -2.0837, 0.1629],
                            [-0.3546, -1.5688, 0.8352, -0.9124, -0.4292, 0.6640],
                            [-0.0590, -0.5079, -1.4143, -2.8996, 2.0022, 2.1226],
                        ],
                        [
                            [-1.3240, -0.7793, -0.7658, 1.7967, -1.0400, 2.4591],
                            [-1.9471, -1.8028, -0.6532, -0.4214, 0.8330, -0.4253],
                            [1.2435, -1.8752, 0.7873, -0.3992, 2.5225, -0.1823],
                            [1.6708, -1.0504, -0.5682, 0.7047, -2.0963, 0.8956],
                            [2.8497, 1.6224, -1.3052, -1.2166, 1.8128, 2.1704],
                            [-0.9109, 0.3209, 0.8719, -1.8299, 0.2371, -1.0139],
                            [-0.2054, 2.2945, 0.2253, -0.9909, -2.0837, 0.1629],
                            [-0.3546, -1.5688, 0.8352, -0.9124, -0.4292, 0.6640],
                            [-0.0590, -0.5079, -1.4143, -2.8996, 2.0022, 2.1226],
                        ],
                    ],
                ]
            )
        ]

        # cls.correct_pred_output = torch.rand(
        #     1,
        #     batch_size,
        #     len(cls.params["quantiles"]) if cls.params["prediction_type"] == 'quantile' else 1,
        #     cls.params["decoder_patch_len"],
        #     cls.params["num_input_channels"],
        # )
        cls.correct_pred_output = torch.tensor(
            [
                [
                    [
                        [[1.0005], [0.9994], [0.9990], [0.9986], [0.9997]],
                        [[0.9992], [0.9995], [0.9993], [0.9981], [0.9976]],
                        [[1.0011], [1.0012], [1.0011], [0.9999], [1.0005]],
                        [[1.0016], [1.0009], [1.0006], [1.0008], [0.9998]],
                        [[1.0005], [1.0013], [1.0030], [1.0023], [1.0027]],
                        [[0.9982], [0.9984], [0.9988], [0.9994], [0.9989]],
                        [[0.9988], [0.9986], [0.9996], [1.0010], [1.0008]],
                        [[1.0001], [0.9987], [0.9991], [0.9997], [0.9985]],
                        [[0.9993], [0.9994], [1.0012], [1.0001], [0.9994]],
                    ],
                    [
                        [[1.0005], [0.9994], [0.9990], [0.9986], [0.9997]],
                        [[0.9992], [0.9995], [0.9993], [0.9981], [0.9976]],
                        [[1.0011], [1.0012], [1.0011], [0.9999], [1.0005]],
                        [[1.0016], [1.0009], [1.0006], [1.0008], [0.9998]],
                        [[1.0005], [1.0013], [1.0030], [1.0023], [1.0027]],
                        [[0.9982], [0.9984], [0.9988], [0.9994], [0.9989]],
                        [[0.9988], [0.9986], [0.9996], [1.0010], [1.0008]],
                        [[1.0001], [0.9987], [0.9991], [0.9997], [0.9985]],
                        [[0.9993], [0.9994], [1.0012], [1.0001], [0.9994]],
                    ],
                ]
            ]
        )

        # cls.correct_forecast_output = torch.rand(
        #     batch_size,
        #     len(cls.params["quantiles"]) if cls.params["prediction_type"] == 'quantile' else 1,
        #     cls.params["prediction_length"],
        #     cls.params["num_input_channels"],
        # )
        cls.correct_forecast_output = torch.tensor(
            [
                [
                    [
                        [1.0005],
                        [0.9994],
                        [0.9990],
                        [0.9986],
                        [0.9996],
                        [1.0005],
                        [0.9990],
                        [0.9991],
                        [0.9983],
                        [0.9992],
                        [0.9983],
                        [0.9991],
                    ],
                    [
                        [0.9992],
                        [0.9995],
                        [0.9993],
                        [0.9981],
                        [0.9976],
                        [0.9989],
                        [0.9992],
                        [0.9987],
                        [0.9976],
                        [0.9967],
                        [0.9976],
                        [0.9967],
                    ],
                    [
                        [1.0011],
                        [1.0012],
                        [1.0011],
                        [0.9999],
                        [1.0005],
                        [1.0010],
                        [1.0014],
                        [1.0010],
                        [0.9996],
                        [1.0009],
                        [0.9995],
                        [1.0009],
                    ],
                    [
                        [1.0016],
                        [1.0009],
                        [1.0006],
                        [1.0008],
                        [0.9998],
                        [1.0028],
                        [1.0013],
                        [1.0010],
                        [1.0012],
                        [0.9999],
                        [1.0012],
                        [0.9999],
                    ],
                    [
                        [1.0005],
                        [1.0013],
                        [1.0030],
                        [1.0023],
                        [1.0027],
                        [1.0008],
                        [1.0018],
                        [1.0040],
                        [1.0027],
                        [1.0033],
                        [1.0028],
                        [1.0033],
                    ],
                    [
                        [0.9982],
                        [0.9984],
                        [0.9988],
                        [0.9994],
                        [0.9989],
                        [0.9984],
                        [0.9988],
                        [0.9992],
                        [0.9999],
                        [0.9989],
                        [1.0000],
                        [0.9989],
                    ],
                    [
                        [0.9989],
                        [0.9986],
                        [0.9996],
                        [1.0010],
                        [1.0008],
                        [0.9992],
                        [0.9988],
                        [0.9998],
                        [1.0017],
                        [1.0003],
                        [1.0018],
                        [1.0003],
                    ],
                    [
                        [1.0001],
                        [0.9988],
                        [0.9991],
                        [0.9997],
                        [0.9985],
                        [1.0005],
                        [0.9992],
                        [0.9994],
                        [0.9994],
                        [0.9986],
                        [0.9994],
                        [0.9986],
                    ],
                    [
                        [0.9993],
                        [0.9994],
                        [1.0013],
                        [1.0001],
                        [0.9994],
                        [0.9990],
                        [0.9996],
                        [1.0018],
                        [1.0000],
                        [0.9985],
                        [1.0000],
                        [0.9985],
                    ],
                ],
                [
                    [
                        [1.0005],
                        [0.9994],
                        [0.9990],
                        [0.9986],
                        [0.9996],
                        [1.0005],
                        [0.9990],
                        [0.9991],
                        [0.9983],
                        [0.9992],
                        [0.9983],
                        [0.9991],
                    ],
                    [
                        [0.9992],
                        [0.9995],
                        [0.9993],
                        [0.9981],
                        [0.9976],
                        [0.9989],
                        [0.9992],
                        [0.9987],
                        [0.9976],
                        [0.9967],
                        [0.9976],
                        [0.9967],
                    ],
                    [
                        [1.0011],
                        [1.0012],
                        [1.0011],
                        [0.9999],
                        [1.0005],
                        [1.0010],
                        [1.0014],
                        [1.0010],
                        [0.9996],
                        [1.0009],
                        [0.9995],
                        [1.0009],
                    ],
                    [
                        [1.0016],
                        [1.0009],
                        [1.0006],
                        [1.0008],
                        [0.9998],
                        [1.0028],
                        [1.0013],
                        [1.0010],
                        [1.0012],
                        [0.9999],
                        [1.0012],
                        [0.9999],
                    ],
                    [
                        [1.0005],
                        [1.0013],
                        [1.0030],
                        [1.0023],
                        [1.0027],
                        [1.0008],
                        [1.0018],
                        [1.0040],
                        [1.0027],
                        [1.0033],
                        [1.0028],
                        [1.0033],
                    ],
                    [
                        [0.9982],
                        [0.9984],
                        [0.9988],
                        [0.9994],
                        [0.9989],
                        [0.9984],
                        [0.9988],
                        [0.9992],
                        [0.9999],
                        [0.9989],
                        [1.0000],
                        [0.9989],
                    ],
                    [
                        [0.9989],
                        [0.9986],
                        [0.9996],
                        [1.0010],
                        [1.0008],
                        [0.9992],
                        [0.9988],
                        [0.9998],
                        [1.0017],
                        [1.0003],
                        [1.0018],
                        [1.0003],
                    ],
                    [
                        [1.0001],
                        [0.9988],
                        [0.9991],
                        [0.9997],
                        [0.9985],
                        [1.0005],
                        [0.9992],
                        [0.9994],
                        [0.9994],
                        [0.9986],
                        [0.9994],
                        [0.9986],
                    ],
                    [
                        [0.9993],
                        [0.9994],
                        [1.0013],
                        [1.0001],
                        [0.9994],
                        [0.9990],
                        [0.9996],
                        [1.0018],
                        [1.0000],
                        [0.9985],
                        [1.0000],
                        [0.9985],
                    ],
                ],
            ]
        )

    def check_module(
        self,
        task,
        params=None,
        input_data=None,
        check_values=False,
    ):
        if input_data is None:
            input_data = self.__class__.constant_data
        config = FlowStateConfig(**params)

        if task == "forecast":
            mdl = FlowStateForPrediction(config)

            target_output = self.__class__.correct_forecast_output
            enc_output_hidden_states = self.__class__.enc_output_forprediction_hidden_states
            enc_output_last_hidden_state = self.__class__.enc_output_forprediction_last_hidden_state
            dec_output_last_hidden_state = self.__class__.dec_output_forprediction_last_hidden_state
            dec_output_hidden_states = self.__class__.dec_output_forprediction_hidden_states

        elif task == "model":
            mdl = FlowStateModel(config)

            target_output = self.__class__.correct_pred_output
            enc_output_hidden_states = self.__class__.enc_output_hidden_states
            enc_output_last_hidden_state = self.__class__.enc_output_last_hidden_state
            dec_output_last_hidden_state = self.__class__.dec_output_last_hidden_state
            dec_output_hidden_states = self.__class__.dec_output_hidden_states
        else:
            raise ValueError(f"Unknown task {task}")

        output = mdl(
            input_data,
        )

        if task == "forecast":
            if not (config.use_return_dict and isinstance(output, FlowStateForPredictionOutput)) and not (
                not config.return_dict and isinstance(output, tuple)
            ):
                raise Exception("Return type of the model was incorrect!")
        elif task == "model":
            if not (config.use_return_dict and isinstance(output, FlowStateModelOutput)) and not (
                not config.return_dict and isinstance(output, tuple)
            ):
                raise Exception("Return type of the model was incorrect!")
        else:
            pass

        def compare(value1, value2, msg=""):
            if not check_values:
                value1, value2 = value1.shape, value2.shape
            torch.testing.assert_close(value1, value2, rtol=TOLERANCE, atol=TOLERANCE, msg=msg)

        if not config.return_dict and task == "forecast":
            output = FlowStateForPredictionOutput(
                loss=output[0],
                prediction_outputs=output[1],  # tensor [batch_size x prediction_length x num_input_channels]
                backbone_hidden_state=output[2],
                decoder_hidden_state=output[3],
                hidden_states=output[4],
            )
        elif not config.return_dict and task == "model":
            output = FlowStateModelOutput(
                last_hidden_state=output[0],
                hidden_states=output[1],
                embedded_input=output[2],
                embedded_output=output[3],
                backbone_hidden_state=output[4],
                decoder_hidden_state=output[5],
            )

        compare(output.backbone_hidden_state, enc_output_last_hidden_state, msg="The encoder outputs do not match!")
        compare(output.decoder_hidden_state, dec_output_last_hidden_state, msg="The decoder outputs do not match!")
        [
            compare(out, tar, msg="The hidden states of the encoder are different!")
            for out, tar in zip(output.hidden_states[:-1], enc_output_hidden_states)
        ]
        [
            compare(out, tar, msg="The hidden states of the decoder are different!")
            for out, tar in zip(output.hidden_states[-1:], dec_output_hidden_states)
        ]

        if task == "forecast":
            compare(output.prediction_outputs, target_output, msg="The final output does not match!")
            self.assertEqual(output.loss, None)
        elif task == "model":
            compare(output.last_hidden_state, target_output, msg="The final output does not match!")
            compare(output.embedded_input, input_data, msg="The input of the embedding does not match!")
            compare(
                output.embedded_output, self.__class__.embed_output, msg="The output of the embedding does not match!"
            )

    @parameterized.expand(
        [
            [True, "forecast", True, False],
            [False, "forecast", True, False],
            [True, "model", True, False],
            [False, "model", True, False],
            [True, "forecast", False, False],
            [False, "forecast", False, False],
            [True, "model", False, False],
            [False, "model", False, False],
            [True, "forecast", True, True],
            [False, "forecast", True, True],
            [True, "model", True, True],
            [False, "model", True, True],
            [True, "forecast", False, True],
            [False, "forecast", False, True],
            [True, "model", False, True],
            [False, "model", False, True],
        ]
    )
    def test_checkshapesandvalues(self, batch_first, task, return_dict, check_values):
        input_data = (
            self.__class__.constant_data if batch_first else torch.transpose(self.__class__.constant_data, 1, 0)
        )

        torch.manual_seed(42)
        torch.cuda.manual_seed_all(42)

        params = self.__class__.params.copy()
        params.update(return_dict=return_dict, batch_first=batch_first)

        self.check_module(task=task, params=params, input_data=input_data, check_values=check_values)
