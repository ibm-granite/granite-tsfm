# Copyright contributors to the TSFM project

# Portions of this Dockerfile are taken from
# https://gitlab.com/nvidia/container-images
# Copyright (c) 2019,2020,2021 NVIDIA CORPORATION. All rights reserved.

FROM tsfminference-cpu AS base

USER root

# Define CUDA version variables as build arguments for easier maintenance
ARG CUDA_MAJOR_VERSION=12
ARG CUDA_MINOR_VERSION=4
ARG CUDA_PATCH_VERSION=1
ARG CUDA_VERSION=${CUDA_MAJOR_VERSION}.${CUDA_MINOR_VERSION}.${CUDA_PATCH_VERSION}
ARG CUDA_PKG_VERSION=${CUDA_MAJOR_VERSION}-${CUDA_MINOR_VERSION}

ARG PYTHON_VERSION=3.12
ARG CODEDIR=tsfminference

ENV NVARCH="x86_64"
# Simplified driver requirements - supports CUDA 12.4+ with driver 470+
ENV NVIDIA_REQUIRE_CUDA="cuda>=${CUDA_MAJOR_VERSION}.${CUDA_MINOR_VERSION}"
ENV NV_CUDA_CUDART_VERSION="12.4.127-1"

COPY cuda.repo-x86_64 /etc/yum.repos.d/cuda.repo
RUN NVIDIA_GPGKEY_SUM=d0664fbbdb8c32356d45de36c5984617217b2d0bef41b93ccecd326ba3b80c87 && \
    curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/rhel9/${NVARCH}/D42D0685.pub | sed '/^Version/d' > /etc/pki/rpm-gpg/RPM-GPG-KEY-NVIDIA && \
    echo "$NVIDIA_GPGKEY_SUM  /etc/pki/rpm-gpg/RPM-GPG-KEY-NVIDIA" | sha256sum -c --strict -

ENV CUDA_VERSION=${CUDA_VERSION}

# For libraries in the cuda-compat-* package: https://docs.nvidia.com/cuda/eula/index.html#attachment-a
RUN microdnf install -y yum && yum install -y \
    cuda-cudart-${CUDA_PKG_VERSION}-${NV_CUDA_CUDART_VERSION} \
    cuda-compat-${CUDA_PKG_VERSION} \
    && yum clean all \
    && microdnf clean all \
    && rm -rf /var/cache/yum/*

# nvidia-docker 1.0
RUN echo "/usr/local/nvidia/lib" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "/usr/local/nvidia/lib64" >> /etc/ld.so.conf.d/nvidia.conf

ENV PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:${PATH}
ENV LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64

# nvidia-container-runtime
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES="compute,utility"

FROM base AS runtime-base

# Re-declare ARGs needed in this stage (ARGs don't carry across stages)
ARG CUDA_MAJOR_VERSION=12
ARG CUDA_MINOR_VERSION=4
ARG CUDA_PKG_VERSION=${CUDA_MAJOR_VERSION}-${CUDA_MINOR_VERSION}
ARG PYTHON_VERSION=3.12
ARG CODEDIR=tsfminference

# Define library versions as build arguments for easier updates
ARG NV_CUDA_LIB_VERSION=12.4.1-1
ARG NV_NVTX_VERSION=12.4.127-1
ARG NV_LIBNPP_VERSION=12.2.5.30-1
ARG NV_LIBCUBLAS_VERSION=12.4.5.8-1
ARG NV_LIBNCCL_PACKAGE_VERSION=2.21.5-1
ARG NV_LIBNCCL_VERSION=2.21.5
ARG NV_CUDNN_VERSION=9.1.0.70-1

ENV NV_CUDA_LIB_VERSION=${NV_CUDA_LIB_VERSION}
ENV NV_NVTX_VERSION=${NV_NVTX_VERSION}
ENV NV_LIBNPP_VERSION=${NV_LIBNPP_VERSION}
ENV NV_LIBNPP_PACKAGE=libnpp-${CUDA_PKG_VERSION}-${NV_LIBNPP_VERSION}
ENV NV_LIBCUBLAS_VERSION=${NV_LIBCUBLAS_VERSION}
ENV NV_LIBNCCL_PACKAGE_NAME=libnccl
ENV NV_LIBNCCL_PACKAGE_VERSION=${NV_LIBNCCL_PACKAGE_VERSION}
ENV NV_LIBNCCL_VERSION=${NV_LIBNCCL_VERSION}
ENV NCCL_VERSION=${NV_LIBNCCL_VERSION}
ENV NV_LIBNCCL_PACKAGE=${NV_LIBNCCL_PACKAGE_NAME}-${NV_LIBNCCL_PACKAGE_VERSION}+cuda${CUDA_MAJOR_VERSION}.${CUDA_MINOR_VERSION}
ENV NV_CUDNN_VERSION=${NV_CUDNN_VERSION}
ENV NV_CUDNN_PACKAGE="libcudnn9-cuda-${CUDA_MAJOR_VERSION}-${NV_CUDNN_VERSION}"
LABEL com.nvidia.cudnn.version="${NV_CUDNN_VERSION}"

RUN yum install -y \
    cuda-libraries-${CUDA_PKG_VERSION}-${NV_CUDA_LIB_VERSION} \
    cuda-nvtx-${CUDA_PKG_VERSION}-${NV_NVTX_VERSION} \
    ${NV_LIBNPP_PACKAGE} \
    libcublas-${CUDA_PKG_VERSION}-${NV_LIBCUBLAS_VERSION} \
    ${NV_LIBNCCL_PACKAGE} \
    ${NV_CUDNN_PACKAGE} \
    && yum clean all \
    && rm -rf /var/cache/yum/*

# Fixed LD_LIBRARY_PATH - removed duplicate paths and double slashes
ENV LD_LIBRARY_PATH=${HOME}/.venv/lib/python${PYTHON_VERSION}/site-packages/nvidia/cuda_cupti/lib:${HOME}/.venv/lib/python${PYTHON_VERSION}/site-packages/nvidia/cusparselt/lib:${LD_LIBRARY_PATH}

COPY --chown=tsfm:tsfm --chmod=755 ${CODEDIR}/* ${HOME}/${CODEDIR}/
COPY --chown=tsfm:tsfm --chmod=755 pyproject.toml uv.lock ${HOME}
USER tsfm

# get back our gpu-enabled version of torch and all the nvidia packages
RUN uv lock && uv sync --locked --reinstall-package torch

ENV CUDA_VISIBLE_DEVICES=0
HEALTHCHECK CMD curl --fail http://localhost:8000/healthcheck || exit 1
