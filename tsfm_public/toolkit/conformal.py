# Copyright contributors to the TSFM project
#
"""Utilities to support conformal forecasts"""

import enum
import json
import logging
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
import torch
from scipy.stats import norm
from transformers.feature_extraction_utils import PreTrainedFeatureExtractor

from tsfm_public.toolkit.processor import STRING_TO_TYPE, TYPE_TO_STRING, BaseProcessor


LOGGER = logging.getLogger(__file__)


class NonconformityScores(enum.Enum):
    """`Enum` for the different kinds of nonconformity scores."""

    ABSOLUTE_ERROR = "absolute_error"
    ERROR = "error"


class PositiveNonconformityScores(enum.Enum):
    """`Enum` for the different kinds of positive nonconformity scores."""

    ABSOLUTE_ERROR = "absolute_error"


class ThresholdFunction(enum.Enum):
    """`Enum` for the different kinds of nonconformity score thresholding functions."""

    WEIGHTING = "weighting"


class Weighting(enum.Enum):
    """`Enum` for the different kinds of nonconformity score weighting approaches."""

    UNIFORM = "uniform"
    EXPONENTIAL_DECAY = "exponential_decay"


class WeightingOptimization(enum.Enum):
    """`Enum` for the different nonconformity score weighting optimization approaches."""

    WASS1 = "wass1"


class PostHocProbabilisticMethod(enum.Enum):
    CONFORMAL = "conformal"
    GAUSSIAN = "gaussian"


"""
Post-Hoc Probabilistic Wrapper Classes
"""


class PostHocProbabilisticProcessor(BaseProcessor):
    """Entry point for posthoc probabilistic approaches for forecasting

    We adopt HF FeatureExtractionMixin through the BaseProcessor class to create the processor and support
    serialization and deserialization

    """

    PROCESSOR_NAME = "conformal_config.json"

    def __init__(
        self,
        window_size: Optional[int] = None,
        quantiles: List[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
        nonconformity_score: str = NonconformityScores.ABSOLUTE_ERROR.value,
        method: str = PostHocProbabilisticMethod.CONFORMAL.value,
        weighting: str = Weighting.UNIFORM.value,
        weighting_params: Dict[str, Any] = {},
        threshold_function: str = ThresholdFunction.WEIGHTING.value,
        aggregation: Union[str, int] = "median",
        aggregation_axis: Union[int, Tuple[int, ...]] = 1,
        id_columns: List[str] = [],
        **kwargs,
    ):
        """
        PostHoc Probabilistic Processor. Turns the point estimates of a multivariate forecast model into probabilistic
        forecasts (quantile estimate of the target variable)

        Args:
            window_size (int, optional): Maximum context window size for considering past residuals. If None
                (default), all previous provided observations are used.
            quantiles (List[float]): List of target quantiles to compute, with values in the open interval (0, 1).
            nonconformity_score (str, optional): Name of the nonconformity score to use, as defined in the
                `NonconformityScores` enum. Applicable only if the method is conformal.
            method (str): Name of the post-hoc probabilistic method to use, as defined in the
                `PostHocProbabilisticMethod` enum.
            weighting (str): Strategy for weighting nonconformity scores, as defined in the `Weighting` enum. Only
                applicable if method = "conformal".
            weighting_params (dict,optional): Parameters for the selected weighting strategy, if applicable. Only applicable
                if method = "conformal".
                Supported keys include:
                - 'optimization' (str, optional): Past nonconformity score weighting optimization method, as defined in the 'WeightingOptimization' enum.
                - 'decay_param' (float, optional): Exponential decay factor used when 'optimization' is set to 'exponential_decay'.
            threshold_function (str): Method for computing the threshold, as defined in the `ThresholdFunction` enum.
                Only applicable if method = "conformal".
            aggregation (str, int, or None, optional):
                Determines how to aggregate the outlier scores across dimensions for the method outlier_score.
                - If a string (e.g., 'mean', 'max', 'min', 'median'), applies the specified aggregation function over the axes provided in `aggregation_axis`.
                - If an integer, selects the outlier scores corresponding to that specific forecast horizon index.
                - If None, returns outlier scores for all dimensions independently without aggregation.
            aggregation_axis (tuple of int, optional) to consider in outlier_score:
                Specifies the axes over which to aggregate when `aggregation` is a string.
            id_columns (List[str]): List of column names which identify different time series in a multi-time series input. Defaults to [].
        """

        self.window_size = window_size
        self.quantiles = quantiles
        self.false_alarm = (np.min(quantiles) * 2).item()
        self.nonconformity_score = nonconformity_score
        self.method = method
        self.aggregation = aggregation
        self.aggregation_axis = aggregation_axis
        self.id_columns = id_columns

        # if self.nonconformity_score in [NonconformityScores.ERROR.value]:
        # self.false_alarm = np.min(quantiles).item()

        self.critical_size = np.ceil(1 / self.false_alarm).item()
        if self.nonconformity_score in [NonconformityScores.ERROR.value]:
            self.critical_size = 2 * self.critical_size

        if self.method not in [PostHocProbabilisticMethod.CONFORMAL.value, PostHocProbabilisticMethod.GAUSSIAN.value]:
            raise ValueError(f"Provided Post Hoc probabilistic method {self.method} is not valid.")

        if self.nonconformity_score not in [
            NonconformityScores.ABSOLUTE_ERROR.value,
            NonconformityScores.ERROR.value,
        ]:
            raise ValueError(f"Provided nonconformity_score {self.nonconformity_score} is not valid.")

        self.weighting = weighting
        self.weighting_params = weighting_params
        self.threshold_function = threshold_function

        self.model = kwargs.pop("model", None)

        if self.model is None:
            if self.method == PostHocProbabilisticMethod.CONFORMAL.value:
                self.model = WeightedConformalForecasterWrapper(
                    window_size=self.window_size,
                    false_alarm=self.false_alarm,
                    nonconformity_score=self.nonconformity_score,
                    weighting=self.weighting,
                    weighting_params=self.weighting_params,
                    threshold_function=self.threshold_function,
                )
            elif self.method == PostHocProbabilisticMethod.GAUSSIAN.value:
                self.model = PostHocGaussian(window_size=self.window_size, quantiles=self.quantiles)
            else:
                raise ValueError(f"Invalid method provided {self.method}")

        kwargs["processor_class"] = self.__class__.__name__
        super().__init__(**kwargs)

    def to_dict(self) -> Dict[str, Any]:
        """
        Serializes this instance to a Python dictionary.

        Returns:
            `Dict[str, Any]`: Dictionary of all the attributes that make up this feature extractor instance.
        """
        output = super().to_dict()

        if isinstance(output["model"], dict):
            model_out = {}
            for k, v in output["model"].items():
                model_out[k] = v.to_dict()
            has_keys = True
        else:
            model_out = output["model"].to_dict()
            has_keys = False

        output["model"] = model_out

        # get type information
        if has_keys:
            akey = next(iter(output["model"].keys()))
            if isinstance(akey, Tuple):
                key_types = [type(k) for k in akey]
            else:
                key_types = [type(akey)]
        else:
            key_types = []

        output["id_columns_types"] = [TYPE_TO_STRING[k] for k in key_types]

        return output

    def to_json_string(self) -> str:
        """
        Serializes this instance to a JSON string.

        Returns:
            `str`: String containing all the attributes that make up this feature_extractor instance in JSON format.
        """
        dictionary = self.to_dict()

        def serialize_np_scalar(value):
            if isinstance(value, np.integer):
                return int(value)
            if isinstance(value, np.floating):
                return float(value)
            return value

        def recursive_check_ndarray(dictionary):
            key_map = {}
            for key, value in dictionary.items():
                if isinstance(key, tuple):
                    new_key = json.dumps([serialize_np_scalar(k) for k in key])
                    key_map[key] = new_key

                if key == "dtype":
                    # to do: ensure deserializable
                    dictionary[key] = value.__name__
                elif isinstance(value, np.ndarray):
                    dictionary[key] = value.tolist()
                elif isinstance(value, (np.integer, np.floating)):
                    dictionary[key] = serialize_np_scalar(value)
                elif isinstance(value, list):
                    dictionary[key] = [vv.tolist() if isinstance(vv, np.ndarray) else vv for vv in value]
                elif isinstance(value, dict):
                    dictionary[key] = recursive_check_ndarray(value)

            for key, new_key in key_map.items():
                dictionary[new_key] = dictionary.pop(key)

            return dictionary

        dictionary = recursive_check_ndarray(dictionary)

        # make sure private name "_processor_class" is correctly
        # saved as "processor_class"
        _processor_class = dictionary.pop("_processor_class", None)
        if _processor_class is not None:
            dictionary["processor_class"] = _processor_class

        return json.dumps(dictionary, indent=2, sort_keys=True) + "\n"

    @classmethod
    def from_dict(cls, feature_extractor_dict: Dict[str, Any], **kwargs) -> "PreTrainedFeatureExtractor":
        """
        Instantiates a type of [`~feature_extraction_utils.FeatureExtractionMixin`] from a Python dictionary of
        parameters.

        Args:
            feature_extractor_dict (`Dict[str, Any]`):
                Dictionary that will be used to instantiate the feature extractor object. Such a dictionary can be
                retrieved from a pretrained checkpoint by leveraging the
                [`~feature_extraction_utils.FeatureExtractionMixin.to_dict`] method.
            kwargs (`Dict[str, Any]`):
                Additional parameters from which to initialize the feature extractor object.

        Returns:
            [`~feature_extraction_utils.FeatureExtractionMixin`]: The feature extractor object instantiated from those
            parameters.
        """

        def deserialize_helper(v, method):
            if method == PostHocProbabilisticMethod.CONFORMAL.value:
                return WeightedConformalForecasterWrapper.from_dict(v)

            if method == PostHocProbabilisticMethod.GAUSSIAN.value:
                return PostHocGaussian.from_dict(v)

            raise ValueError(f"Unknown method provided: {method}")

        def deserialize_key(key, key_types=None):
            if not key_types:
                return key

            key = json.loads(key)
            if isinstance(key, (Tuple, List)):
                return tuple([STRING_TO_TYPE[k_type](k_item) for k_item, k_type in zip(key, key_types)])
            else:
                return STRING_TO_TYPE[key_types[0]](key)

        id_types = feature_extractor_dict.get("id_columns_types", None)

        model = feature_extractor_dict.get("model", None)
        if model is not None:
            method = feature_extractor_dict.get("method", None)
            if id_types:
                feature_extractor_dict["model"] = {}
                for k, v in model.items():
                    feature_extractor_dict["model"][deserialize_key(k, key_types=id_types)] = deserialize_helper(
                        v, method
                    )
            else:
                feature_extractor_dict["model"] = deserialize_helper(model, method)

        return super().from_dict(feature_extractor_dict, **kwargs)

    @classmethod
    def _get_numpy_input(cls, df: pd.DataFrame, id_columns: List[str] = []) -> np.ndarray:
        """Convert dataframe output from the forecasting pipeline in the numpy array format needed
        for conformal.

        Args:
            df (pd.DataFrame): Input dataframe from the output of the forecasting pipeline.

        Returns:
            np.ndarray: Tensor with the following shape: number of samples x prediction length x number of features
        """

        columns = [c for c in df.columns if c not in id_columns]

        return np.array([np.stack(z) for z in df[columns].values]).transpose(0, 2, 1)

    @classmethod
    def _get_unique_id_values(
        cls,
        data: Union[np.ndarray, pd.DataFrame],
        id_column_values: Optional[np.ndarray] = None,
        id_columns: List[str] = [],
    ) -> Union[None, np.ndarray]:
        """Get unique id column values from the data.

        Args:
            data (Union[np.ndarray, pd.DataFrame]): Either of the following:
                Dataframe of input data including data for id_columns, in which case id_columns will be used to select the columns.
                Ndarry of input data, in which case id_column_values should be passed.
            id_column_values:(np.ndarray, optional): Array of id_column values.
            id_columns (List[str], optional): ID columns used to select columns when dataframe input is given. Defaults to [].

        Raises:
            ValueError: When neither ndarray or dataframe input are given

        Returns:
            Union[None, np.ndarray]: Unique combinations of the id_column values extracted from data. None in the case
                where no id_columns are specficied and dataframe input is given.
        """

        if isinstance(data, pd.DataFrame) and id_column_values is not None:
            raise ValueError("Recevied dataframe input and ndarray of id_column_values")

        if id_column_values is not None:
            id_data = pd.DataFrame(id_column_values)
        else:
            if not id_columns:
                return None
            id_data = data[id_columns]

        return id_data.drop_duplicates().values

    def train(
        self,
        y_cal_gt: Union[pd.DataFrame, np.ndarray],
        y_cal_pred: Union[pd.DataFrame, np.ndarray],
        id_column_values: Optional[np.ndarray] = None,
    ) -> "PostHocProbabilisticProcessor":
        """Fit posthoc probabilistic wrapper.

        Args:
            y_cal_gt (Union[pd.DataFrame, np.ndarray]):  ground truth values, nsamples x forecast_horizon x number_features
            y_cal_pred (Union[pd.DataFrame, np.ndarray]): model perdictions, nsamples x forecast_horizon x number_features
            id_column_values: (np.ndarray, optional): array of id column values, used for handling multi-series input only when numpy arrays
                are passed above
        """

        unique_ids = self._get_unique_id_values(
            data=y_cal_pred, id_column_values=id_column_values, id_columns=self.id_columns
        )

        if isinstance(y_cal_pred, pd.DataFrame):
            id_column_values = y_cal_pred[self.id_columns].values if self.id_columns else None
            y_cal_pred = self._get_numpy_input(y_cal_pred, id_columns=self.id_columns)

        if isinstance(y_cal_gt, pd.DataFrame):
            y_cal_gt = self._get_numpy_input(y_cal_gt, id_columns=self.id_columns)

        if (isinstance(y_cal_pred, pd.DataFrame) and not isinstance(y_cal_gt, pd.DataFrame)) or (
            not isinstance(y_cal_pred, pd.DataFrame) and isinstance(y_cal_gt, pd.DataFrame)
        ):
            raise ValueError("When one of y_cal_gt or y_cal_pred is a dataframe both are expected to be a dataframe.")

        # to do?: ensure that y_cal_pred / y_cal_gt are ordered similarly (same dim and same id column values)

        if len(y_cal_pred.shape) != 3:
            raise ValueError("y_cal_pred should have 3 dimensions: nsamples x forecast_horizon x number_features")

        if len(y_cal_gt.shape) != 3:
            raise ValueError("y_cal_gt should have 3 dimensions: nsamples x forecast_horizon x number_features")

        if id_column_values is None:
            self.model.fit(
                y_cal_gt=y_cal_gt,
                y_cal_pred=y_cal_pred,  # ttm predicted values
                # ttm corresponding gt values
            )
        else:
            ### Initialize and fit a dictionary of models ###
            self.model = {}
            for g in unique_ids:
                idx = np.where(np.all(id_column_values == g, axis=1))[0]  # indices for this group
                g = tuple(g)  # tuple(gg.item() for gg in g)
                y_cal_pred_g, y_cal_gt_g = y_cal_pred[idx], y_cal_gt[idx]
                if y_cal_pred_g.shape[0] < self.critical_size:
                    raise ValueError(
                        f"id {g} has only {y_cal_pred_g.shape[0]} rows of data which is less than the minimum number {self.critical_size}"
                    )
                if self.method == PostHocProbabilisticMethod.CONFORMAL.value:
                    self.model[g] = WeightedConformalForecasterWrapper(
                        window_size=self.window_size,
                        false_alarm=self.false_alarm,
                        nonconformity_score=self.nonconformity_score,
                        weighting=self.weighting,
                        weighting_params=self.weighting_params,
                        threshold_function=self.threshold_function,
                    )
                elif self.method == PostHocProbabilisticMethod.GAUSSIAN.value:
                    self.model[g] = PostHocGaussian(window_size=self.window_size, quantiles=self.quantiles)
                self.model[g].fit(
                    y_cal_gt=y_cal_gt_g,
                    y_cal_pred=y_cal_pred_g,
                )
        return self

    def predict(
        self,
        y_test_pred: Union[pd.DataFrame, np.ndarray],
        quantiles: List[float] = [],
        id_column_values: Optional[np.ndarray] = None,
    ) -> np.ndarray:
        """Predict posthoc probabilistic wrapper.

        Args:
            y_test_pred (Union[pd.DataFrame, np.ndarray]): nsamples x forecast_horizon x number_features
            id_column_values: (np.ndarray, optional): array of id column values, used for handling multi-series input only when numpy arrays
                are passed above

        Returns:
            y_test_prob_pred (np.ndarray): nsamples x forecast_horizon x number_features x len(quantiles)
        """

        unique_ids = self._get_unique_id_values(
            data=y_test_pred, id_column_values=id_column_values, id_columns=self.id_columns
        )

        if isinstance(y_test_pred, pd.DataFrame):
            id_column_values = y_test_pred[self.id_columns].values if self.id_columns else None
            y_test_pred = self._get_numpy_input(y_test_pred, id_columns=self.id_columns)

        if len(quantiles) == 0:
            quantiles = self.quantiles

        assert (
            len(y_test_pred.shape) == 3
        ), " y_test_pred should have 3 dimensions : nsamples x forecast_horizon x number_features"

        y_test_prob_pred = np.zeros([y_test_pred.shape[0], y_test_pred.shape[1], y_test_pred.shape[2], len(quantiles)])
        if id_column_values is None:
            assert not isinstance(
                self.model, dict
            ), "Probabilistic processor models were trained with id_columns and no id_column was provided as an input. Please provide id_columns."
            groups_unique_dummy = [None]
        else:
            groups_unique_dummy = unique_ids
        for g in groups_unique_dummy:
            if id_column_values is None:
                model = self.model
                y_test_pred_g = y_test_pred
                idx = np.arange(y_test_prob_pred.shape[0])
            else:
                idx = np.where(np.all(id_column_values == g, axis=1))[0]
                g = tuple(g)  # tuple(gg.item() for gg in g)
                model = self.model[g]
                y_test_pred_g = y_test_pred[idx]
            if self.method == PostHocProbabilisticMethod.CONFORMAL.value:
                ix_q = 0
                # print('QUANTILES : ', quantiles, self.model.false_alarm)
                # print('NON CONFORMITY SCORE :', self.model.nonconformity_score)
                for q in quantiles:
                    # print(q)
                    if model.nonconformity_score in [
                        NonconformityScores.ABSOLUTE_ERROR.value,
                        NonconformityScores.ERROR.value,
                    ]:
                        if q < 0.5:
                            q_pi_error_rate = q * 2
                            output_q = model.predict(y_test_pred_g, false_alarm=q_pi_error_rate)
                            y_test_prob_pred[idx, :, :, ix_q] = output_q["prediction_interval"]["y_low"]
                        elif q > 0.5:
                            q_pi_error_rate = (1 - q) * 2
                            output_q = model.predict(y_test_pred_g, false_alarm=q_pi_error_rate)
                            y_test_prob_pred[idx, :, :, ix_q] = output_q["prediction_interval"]["y_high"]
                        else:
                            if model.nonconformity_score in [NonconformityScores.ERROR.value]:
                                q_pi_error_rate = 0.5
                                # print('Quantile q = ',q_pi_error_rate)
                                output_q = model.predict(y_test_pred_g, false_alarm=2 * q_pi_error_rate)
                                y_test_prob_pred[idx, :, :, ix_q] = output_q["prediction_interval"]["y_high"]
                            else:
                                y_test_prob_pred[idx, :, :, ix_q] = y_test_pred_g

                    ix_q += 1
            if self.method == PostHocProbabilisticMethod.GAUSSIAN.value:
                y_test_prob_pred[idx, ...] = model.predict(y_test_pred_g)

        return y_test_prob_pred

    def update(
        self,
        y_gt: Union[pd.DataFrame, np.ndarray],
        y_pred: Union[pd.DataFrame, np.ndarray],
        id_column_values: Optional[np.ndarray] = None,
        X: np.ndarray = None,
        timestamps: np.ndarray = None,
    ) -> "PostHocProbabilisticProcessor":
        """
        Update the probabilistic post hoc model

        Args:
            y_gt (Union[pd.DataFrame, np.ndarray]): Ground truth values. Shape: (n_samples, forecast_length, num_features).
            y_pred (Union[pd.DataFrame, np.ndarray]): Predicted values. Shape: (n_samples,forecast_length, num_features).
            id_column_values: (np.ndarray, optional): array of id column values, used for handling multi-series input only when numpy arrays
                are passed above
            X (np.ndarray, optional): Input covariates for input-dependent methods. Shape: (n_samples, n_features).
            timestamps (np.ndarray, optional): Timestamps associated with each predicted value. Shape: (n_samples,).
        """

        unique_ids = self._get_unique_id_values(
            data=y_gt, id_column_values=id_column_values, id_columns=self.id_columns
        )

        if isinstance(y_gt, pd.DataFrame):
            id_column_values = y_gt[self.id_columns].values if self.id_columns else None
            y_gt = self._get_numpy_input(y_gt, id_columns=self.id_columns)
        if isinstance(y_pred, pd.DataFrame):
            y_pred = self._get_numpy_input(y_pred, id_columns=self.id_columns)

        if id_column_values is None:
            if self.method == PostHocProbabilisticMethod.CONFORMAL.value:
                self.model.update(y_gt=y_gt, y_pred=y_pred, X=X, timestamps=timestamps)
            if self.method == PostHocProbabilisticMethod.GAUSSIAN.value:
                self.model.update(y_gt=y_gt, y_pred=y_pred)
        else:
            for g in unique_ids:
                idx = np.where(np.all(id_column_values == g, axis=1))[0]
                y_pred_g, y_gt_g = y_pred[idx], y_gt[idx]
                X_g = X
                timestamps_g = timestamps
                if X is not None:
                    X_g = X[idx]
                if timestamps is not None:
                    timestamps_g = timestamps[idx]

                self.model.update(y_gt=y_gt_g, y_pred=y_pred_g, X=X_g, timestamps=timestamps_g)

        return self

    def outlier_score(
        self,
        y_gt: np.ndarray,
        y_pred: np.ndarray,
        X: np.ndarray = None,
        timestamps: np.ndarray = None,
        aggregation: Union[str, int] = -1,
        significance: float = 0.01,
        aggregation_axis: Union[int, Tuple[int, ...]] = -1,
        outlier_label: bool = True,
    ) -> np.ndarray:
        """
        PROTOTYPE: METHOD TO GET NORMALIZED OUTLIER CONFORMAL SCORE (P-VALUE) BASED ON FORECASTED PREDICTION ERRORS

        Args:
            y_gt (np.ndarray): Ground truth values. Shape: (n_samples, forecast_length, num_features).
            y_pred (np.ndarray): Predicted values. Shape: (n_samples,forecast_length, num_features).
            X (np.ndarray, optional): Input covariates for input-dependent methods. Shape: (n_samples, n_features).
            timestamps (np.ndarray, optional): Timestamps associated with each predicted value. Shape: (n_samples,).
            aggregation (str, int, or None, optional):
                Determines how to aggregate the outlier scores across dimensions.
                - If a string (e.g., 'mean', 'max', 'min', 'median'), applies the specified aggregation function over the axes provided in `aggregation_axis`.
                - If an integer, selects the outlier scores corresponding to that specific forecast horizon index.
                - If None, returns outlier scores for all dimensions independently without aggregation.
                - If -1 sets default aggregation in self.aggreggation attribute

            aggregation_axis (tuple of int, optional):
                Specifies the axes over which to aggregate when `aggregation` is a string.
        Returns:
            output_array (np.ndarray): The resulting array after aggregation. Its shape depends on the chosen aggregation strategy, but the last dimension is always 2, representing the outlier score (p-value) and the outlier label.
        """
        if self.method == PostHocProbabilisticMethod.CONFORMAL.value:
            if self.model.nonconformity_score in [
                NonconformityScores.ABSOLUTE_ERROR.value,
                NonconformityScores.ERROR.value,
            ]:
                output = self.model.predict(
                    y_pred=y_pred, y_gt=y_gt, X=X, timestamps=timestamps, false_alarm=significance
                )
                outliers_scores = output["outliers_scores"]

                """
                Aggregation
                """
                if aggregation == -1:
                    aggregation = self.aggregation
                if aggregation_axis == -1:
                    aggregation_axis = self.aggregation_axis

                if aggregation is None:
                    outliers_scores = self.forecast_horizon_aggregation(
                        outliers_scores, aggregation=aggregation
                    )  # aligns forecast for the same sample
                    filter = np.isnan(outliers_scores)
                    outliers = np.array(np.array(outliers_scores) <= significance).astype("float")
                    outliers[filter] = np.nan

                elif isinstance(aggregation, int):
                    outliers_scores = self.forecast_horizon_aggregation(outliers_scores, aggregation=aggregation)
                    # outliers_scores = outliers_scores[:, aggregation, :]
                    outliers = np.array(np.array(outliers_scores) <= significance).astype("int")

                elif isinstance(aggregation, str):
                    if aggregation_axis is None:
                        raise ValueError("aggregation_axis must be specified when aggregation is a string")

                    if isinstance(aggregation_axis, int):
                        aggregation_axis = (aggregation_axis,)

                    ### If aggregation across forecast horizon (axis = 1) was selected
                    if 1 in aggregation_axis:
                        aggregation_axis = tuple(x for x in aggregation_axis if x != 1)
                        outliers_scores = self.forecast_horizon_aggregation(outliers_scores, aggregation=aggregation)
                        if aggregation_axis:
                            outliers_scores = outliers_scores[:, np.newaxis, :]

                    ### If aggreagtion axis for other dimension != 1 were selected
                    if aggregation_axis:
                        if aggregation == "mean":
                            outliers_scores = np.mean(outliers_scores, axis=aggregation_axis)
                        elif aggregation == "median":
                            outliers_scores = np.median(outliers_scores, axis=aggregation_axis)
                        elif aggregation == "max":
                            outliers_scores = np.max(outliers_scores, axis=aggregation_axis)
                        elif aggregation == "min":
                            outliers_scores = np.min(outliers_scores, axis=aggregation_axis)
                        else:
                            raise ValueError(f"Unsupported aggregation method: {aggregation}")
                    outliers = np.array(np.array(outliers_scores) <= significance).astype("int")
                else:
                    raise TypeError("aggregation must be either an int or a supported aggregation string")

                if outlier_label:
                    return np.stack([outliers_scores, outliers], axis=-1)
                else:
                    return outliers_scores

    def forecast_horizon_aggregation(
        self, outliers_scores: np.ndarray, aggregation: Union[str, int, None] = "mean"
    ) -> np.ndarray:
        N, H, F = outliers_scores.shape
        # we want to align predictions/forecasts for each timestamp (observation)
        aligned = np.full(
            (N, H, F), np.nan
        )  # nan for padding initial items for which we have less than horizon H predictions.
        for h in range(np.minimum(H, N)):
            # shift each row of forecast horizon h by h steps into the future.
            aligned[h:N, h, :] = outliers_scores[
                : N - h, h, :
            ]  # item i should have a values for [i,j,:] for j \in [0, min(i,H)]

        if isinstance(aggregation, int):  ## choose a particular forecast horizon score
            out = aligned[:, aggregation, :]
            for h in range(aggregation + 1):  # aligned in an upper triangular NaNs matrix
                out[h, :] = aligned[h, h, :]
            return out

        elif isinstance(aggregation, str):
            # aggregation but ignore nans
            if aggregation == "mean":
                return np.nanmean(aligned, axis=1)
            elif aggregation == "median":
                return np.nanmedian(aligned, axis=1)
            elif aggregation == "max":
                return np.nanmax(aligned, axis=1)
            elif aggregation == "min":
                return np.nanmin(aligned, axis=1)
            else:
                raise ValueError(f"Unsupported aggregation method: {aggregation}")
        elif aggregation is None:  # just return aligned arrays
            return aligned
        else:
            raise TypeError("aggregation must be either an int or a supported aggregation string")


def absolute_error(y: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
    """
    Compute the absolute error between `y` and `y_pred`. If the inputs are multi-dimensional, the average is computed over the last dimension.

    Args:
        y (np.ndarray): Ground truth values.
        y_pred (np.ndarray): Predicted values.

    Returns:
        np.ndarray: Absolute error. If inputs are multi-dimensional, returns the mean absolute error over the last axis.
    """

    assert y.shape == y_pred.shape, (
        "Shapes of y and y_pred do not match: y.shape, y_pred.shape = " + str(y.shape) + ";" + str(y_pred.shape)
    )
    error = np.abs(y - y_pred)
    if len(error.shape) > 1:
        error = np.mean(error, axis=-1)
    return error


def error(y: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
    """
    Compute the difference between `y` and `y_pred` (signed difference y - y_pred). If the inputs are multi-dimensional, the average is computed over the last dimension.

    Args:
        y (np.ndarray): Ground truth values.
        y_pred (np.ndarray): Predicted values.

    Returns:
        np.ndarray: signed error. If inputs are multi-dimensional, returns the mean absolute error over the last axis.
    """
    assert y.shape == y_pred.shape, (
        "Shapes of y and y_pred do not match: y.shape, y_pred.shape = " + str(y.shape) + ";" + str(y_pred.shape)
    )
    error = y - y_pred
    if len(error.shape) > 1:
        error = np.mean(error, axis=-1)
    return error


def nonconformity_score_functions(
    y_gt: np.ndarray,
    y_pred: np.ndarray,
    X: Optional[np.ndarray] = None,
    nonconformity_score: str = NonconformityScores.ABSOLUTE_ERROR.value,
) -> np.ndarray:
    """
    Compute a predictive nonconformity score between ground truth and predictions with optional input covariates.

    Args:
        y_gt (np.ndarray): Ground truth values.
        y_pred (np.ndarray): Predicted values.
        X (np.ndarray, optional): Input covariates. Used by some nonconformity scoring methods.
        nonconformity_score (str, optional): Type of nonconformity score to compute, as defined in the `NonconformityScores` enum.

    Returns:
        np.ndarray: Computed nonconformity scores.
    """
    assert nonconformity_score in [
        s.value for s in NonconformityScores
    ], "Selected nonconformity score is not supported, choose from {}".format(
        str([s.value for s in NonconformityScores])
    )
    if nonconformity_score == NonconformityScores.ABSOLUTE_ERROR.value:
        return absolute_error(y_gt, y_pred)
    if nonconformity_score == NonconformityScores.ERROR.value:
        return error(y_gt, y_pred)


def conformal_set(
    y_pred: np.ndarray,
    score_threshold: Union[float, List[float]],
    nonconformity_score: str = NonconformityScores.ABSOLUTE_ERROR.value,
) -> Dict[str, np.ndarray]:
    """
    Compute the conformal prediction set or interval for a given prediction.

    Args:
        y_pred (np.ndarray): Predicted values.
        score_threshold (float or List[float]): Quantile(s) of the nonconformity scores used to determine the prediction intervals.
        nonconformity_score (str, optional): Type of nonconformity score to apply, as defined in the `NonconformityScores` enum.

    Returns:
        Dict[str, np.ndarray]: A dictionary containing the lower and upper bounds of the prediction interval or set. Keys are typically 'y_low' and 'y_high' for prediction intervals.
    """

    if nonconformity_score == NonconformityScores.ABSOLUTE_ERROR.value:
        return {"y_low": y_pred - score_threshold, "y_high": y_pred + score_threshold}
    if nonconformity_score == NonconformityScores.ERROR.value:
        return {"y_low": score_threshold[0] + y_pred, "y_high": score_threshold[1] + y_pred}


class PostHocGaussian:
    def __init__(self, window_size=None, quantiles=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]):
        self.window_size = window_size
        self.quantiles = quantiles
        self.critical_size = 1
        self.variance = None
        self.errors = None  # stores the errors, needed for the online updates

        """
        PostHoc Probabilistic Gaussian Wrapper.

        Transforms the point forecasts of a multivariate model into probabilistic forecasts under the assumption of independent Gaussian residuals.

        Args:
            window_size (int, optional): Maximum number of past residuals to consider when estimating variance. If None (default), all available past residuals are used.
            quantiles (List[float]): List of target quantiles to compute. Each value must lie in the open interval (0, 1).
        """

    def fit(self, y_cal_gt: np.ndarray, y_cal_pred: np.ndarray):
        """
        Fit the PostHoc Probabilistic Gaussian Wrapper.

        Args:
            y_cal_gt (np.ndarray): Ground truth values used for calibration.  Shape: (n_samples, forecast_horizon, num_features).
            y_cal_pred (np.ndarray): Model predictions corresponding to the ground truth. Shape: (n_samples, forecast_horizon, num_features).
        """

        if self.window_size is None:
            window_size = y_cal_gt.shape[0]
        else:
            window_size = self.window_size
        assert (
            len(y_cal_pred.shape) == 3
        ), " y_cal_pred should have 3 dimensions : nsamples x forecast_horizon x number_features"
        assert (
            len(y_cal_gt.shape) == 3
        ), " y_cal_gt should have 3 dimensions : nsamples x forecast_horizon x number_features"

        self.errors = y_cal_gt[-window_size:] - y_cal_pred[-window_size:]
        self.variance = np.sum(self.errors**2, axis=0) / (len(self.errors) - 1)  # dimension should be

    def update(self, y_gt: np.ndarray, y_pred: np.ndarray):
        """
        Update the PostHoc Probabilistic Gaussian Wrapper.

        Args:
            y_cal_gt (np.ndarray): Ground truth values used for calibration.  Shape: (n_samples, forecast_horizon, num_features).
            y_cal_pred (np.ndarray): Model predictions corresponding to the ground truth. Shape: (n_samples, forecast_horizon, num_features).
        """
        errors = y_gt - y_pred
        self.errors = np.concatenate([self.errors, errors], axis=0)
        self.errors = self.errors[-self.window_size :]
        self.variance = np.sum(self.errors**2, axis=0) / (len(self.errors) - 1)

    def predict(self, y_test_pred: np.ndarray, quantiles=[]):
        """
        Predict using the PostHoc Probabilistic Gaussian Wrapper.

        Args:
            y_test_pred (np.ndarray): Model point predictions. Shape: (n_samples, forecast_horizon, num_features).

        Returns:
            np.ndarray: Quantile estimates for each prediction. Shape: (n_samples, forecast_horizon, num_features, len(quantiles)).
        """

        if len(quantiles) == 0:
            quantiles = self.quantiles
        assert self.variance is not None, "Method needs to be fitted"
        assert (
            len(y_test_pred.shape) == 3
        ), " y_test_pred should have 3 dimensions: nsamples x forecast_horizon x number_features"
        std_devs = np.sqrt(self.variance)[..., None]  # Standard deviation for each distribution
        quantiles = norm.ppf(quantiles) * std_devs  # Compute quantiles
        y_test_prob_pred = y_test_pred[..., np.newaxis] + quantiles[np.newaxis, ...]
        return y_test_prob_pred

    def to_dict(self) -> Dict[str, Any]:
        """
        Serializes this instance to a Python dictionary.

        Returns:
            `Dict[str, Any]`: Dictionary of all the attributes that make up this object.
        """

        output = {
            "window_size": self.window_size,
            "quantiles": self.quantiles,
            "critical_size": self.critical_size,
            "variance": self.variance,
        }

        return output

    @classmethod
    def from_dict(cls, params: Dict[str, Any], **kwargs) -> "PostHocGaussian":
        """
        Instantiates a type of [`~PostHocGaussian`] from a Python dictionary of
        parameters.

        Args:
            dict (`Dict[str, Any]`):
                Dictionary that will be used to instantiate the object.
            kwargs (`Dict[str, Any]`):
                Additional parameters from which to initialize the object.

        Returns:
            [`~PostHocGaussian`]: The PostHocGaussian object instantiated from those
            parameters.
        """

        variance = params.pop("variance", None)
        critical_size = params.pop("critical_size", 1)
        obj = cls(**params)
        obj.variance = variance
        obj.critical_size = critical_size
        return obj


class WeightedConformalForecasterWrapper:
    def __init__(
        self,
        nonconformity_score: str = NonconformityScores.ABSOLUTE_ERROR.value,
        false_alarm: float = 0.05,
        weighting: str = Weighting.UNIFORM.value,
        weighting_params: Dict[str, Any] = {},
        threshold_function: str = ThresholdFunction.WEIGHTING.value,
        window_size: Optional[int] = None,
    ):
        self.nonconformity_score = nonconformity_score
        # self.nonconformity_score_func = nonconformity_score_functions

        self.quantile = 1 - false_alarm
        self.false_alarm = false_alarm

        self.weighting = weighting
        self.weighting_params = weighting_params
        self.window_size = window_size
        self.online_size = 1
        self.threshold_function = threshold_function
        self.weights_adaptive = []
        self.univariate_wrappers = {}

        """
        Weighted Split Conformal Forecasting Wrapper Class.

        Args:
            nonconformity_score (str): Type of nonconformity score to use, as defined in the `NonconformityScores` enum.
            false_alarm (float): Desired false alarm (error) rate for the prediction intervals.
            weighting (str): Strategy for weighting nonconformity scores, as defined in the `Weighting` enum.
            weighting_params (dict): Parameters for the selected weighting strategy, if applicable.
            threshold_function (str): Method for computing the threshold, as defined in the `ThresholdFunction` enum.
            window_size (int, optional): Maximum number of past nonconformity scores to use for calibration. Default is None (use all available scores).
        """

    def to_dict(self) -> Dict[str, Any]:
        """
        Serializes this instance to a Python dictionary.

        Returns:
            `Dict[str, Any]`: Dictionary of all the attributes that make up this object.
        """

        output = {
            "nonconformity_score": self.nonconformity_score,
            "false_alarm": self.false_alarm,
            "weighting": self.weighting,
            "weighting_params": self.weighting_params,
            "window_size": self.window_size,
            "threshold_function": self.threshold_function,
            "weights_adaptive": self.weights_adaptive,
            "univariate_wrappers": {json.dumps(k): v.to_dict() for k, v in self.univariate_wrappers.items()},
        }

        return output

    @classmethod
    def from_dict(cls, params: Dict[str, Any], **kwargs) -> "WeightedConformalForecasterWrapper":
        """
        Instantiates a type of [`~WeightedConformalForecasterWrapper`] from a Python dictionary of
        parameters.

        Args:
            dict (`Dict[str, Any]`):
                Dictionary that will be used to instantiate the object.
            kwargs (`Dict[str, Any]`):
                Additional parameters from which to initialize the object.

        Returns:
            [`~WeightedConformalForecasterWrapper`]: The WeightedcCnformalForecaster object instantiated from those
            parameters.
        """

        params_copy = params.copy()

        save_attrs = ["weights_adaptive", "univariate_wrappers"]
        copy_attrs = ["weights_adaptive"]

        save_attrs_dict = {}
        for attr in save_attrs:
            save_attrs_dict[attr] = params_copy.pop(attr)

        obj = cls(**params_copy, **kwargs)

        for attr_name, attr_value in save_attrs_dict.items():
            if attr_name in copy_attrs:
                setattr(obj, attr_name, attr_value)

        univariate_wrappers = {}
        for k, v in save_attrs_dict["univariate_wrappers"].items():
            k_decoded = tuple(json.loads(k))
            univariate_wrappers[k_decoded] = WeightedConformalWrapper.from_dict(v)

        obj.univariate_wrappers = univariate_wrappers

        return obj

    def fit(
        self, y_cal_gt: np.ndarray, y_cal_pred: np.ndarray, X_cal: np.ndarray = None, cal_timestamps: np.ndarray = None
    ):
        """
        Fit the Weighted Split Conformal Forecasting method.

        Args:
            y_cal_gt (np.ndarray): Ground truth values. Shape: (num_samples, forecast_length, num_features).
            y_cal_pred (np.ndarray): Forecasted values from the time series model. Shape: (num_samples, forecast_length, num_features).
            X_cal (np.ndarray, optional): Input covariates for input-dependent conformal methods. Shape: (num_samples, num_covariates).
            cal_timestamps (np.ndarray, optional): Timestamps associated with each forecasted value. Shape: (num_samples, forecast_length).
        """

        if self.window_size is None:
            self.window_size = y_cal_pred.shape[0]

        window_critical_size = int(np.ceil(1 / self.false_alarm))
        if (self.window_size < window_critical_size) and (y_cal_pred.shape[0] >= window_critical_size):
            self.window_size = window_critical_size

        if self.window_size < window_critical_size:
            raise ValueError(f"""Not enough calibration points for the
                             desired error rate. For an error rate of {self.false_alarm}
                             we need at least {window_critical_size} calibration points""")

        self.univariate_wrappers = {}
        for ix_f in range(y_cal_pred.shape[2]):
            cal_weights = None

            """
            Weights Optimization
            """
            # Copy the dictionary
            weighting_params = self.weighting_params.copy()
            if "optimization" in self.weighting_params.keys():
                if self.weighting_params["optimization"] == WeightingOptimization.WASS1.value:  # "wass1":
                    weighting_params.pop("optimization", None)
                    """
                    Fitting Weights
                    1. compute non-conformity scores.3
                    2. Initialize Wass1 weight adapter
                    """
                    critical_efficient_size = int(np.ceil(1 / self.false_alarm))
                    cal_scores = nonconformity_score_functions(
                        y_cal_gt[:, :, ix_f],
                        y_cal_pred[:, :, ix_f],
                        X=X_cal,
                        nonconformity_score=self.nonconformity_score,
                    )

                    stride = 1
                    n_batch_update = int((critical_efficient_size + 2))
                    n_updates = 100
                    n_cal_optimization = (n_updates - 1) * stride + n_batch_update
                    n_cal_init = np.maximum(int(critical_efficient_size) * 2, cal_scores.shape[0] - n_cal_optimization)
                    lr = 0.001

                    if cal_scores.shape[0] >= n_cal_init + n_batch_update:
                        awcsw = AdaptiveWeightedConformalScoreWrapper(
                            false_alarm=self.false_alarm,
                            window_size=self.window_size,
                            weighting=Weighting.UNIFORM.value,
                            weighting_params={
                                "n_batch_update": n_batch_update,
                                "conformal_weights_update": False,
                                "stride": stride,
                                "lr": lr,
                            },
                        )

                        # Need to call these because of side effects
                        _ = awcsw.fit(cal_scores[0 : int(n_cal_init)])  # betas_fit
                        _ = awcsw.predict(cal_scores[int(n_cal_init) :])  # betas_update
                        cal_weights = awcsw.cal_weights

            for ix_h in range(y_cal_pred.shape[1]):
                cal_timestamps_i = None
                if cal_timestamps is not None:
                    cal_timestamps_i = cal_timestamps[:, ix_h, ix_f]
                self.univariate_wrappers[ix_h, ix_f] = WeightedConformalWrapper(
                    nonconformity_score=self.nonconformity_score,
                    false_alarm=self.false_alarm,
                    weighting=self.weighting,
                    weighting_params=weighting_params,
                    threshold_function=self.threshold_function,
                    window_size=self.window_size,
                    online_size=self.online_size,
                )
                self.univariate_wrappers[ix_h, ix_f].fit(
                    y_cal_gt[:, ix_h, ix_f], y_cal_pred[:, ix_h, ix_f], X_cal=X_cal, cal_timestamps=cal_timestamps_i
                )
                if cal_weights is not None:
                    self.univariate_wrappers[ix_h, ix_f].weights.append(cal_weights)

    def update(self, y_gt: np.ndarray, y_pred: np.ndarray, X: np.ndarray = None, timestamps: np.ndarray = None):
        """
        Update the nonconformity scores and threshold function.

        Args:
            y_gt (np.ndarray): Ground truth values. Shape: (n_samples, forecast_length, num_features).
            y_pred (np.ndarray): Predicted values. Shape: (n_samples,forecast_length, num_features).
            X (np.ndarray, optional): Input covariates for input-dependent methods. Shape: (n_samples, n_features).
            timestamps (np.ndarray, optional): Timestamps associated with each predicted value. Shape: (n_samples,).
        """
        # print(y_pred.shape, y_gt.shape)
        for ix_h in range(y_pred.shape[1]):
            for ix_f in range(y_pred.shape[2]):
                timestamps_i = None
                if timestamps is not None:
                    timestamps_i = timestamps[:, ix_h, ix_f]

                y_gt_i = y_gt[:, ix_h, ix_f]

                _ = self.univariate_wrappers[ix_h, ix_f].predict(
                    y_pred[:, ix_h, ix_f], y_gt=y_gt_i, X=X, timestamps=timestamps_i, update=True
                )

    def predict(
        self,
        y_pred: np.ndarray,
        y_gt: np.ndarray = None,
        X: np.ndarray = None,
        timestamps: np.ndarray = None,
        false_alarm: float = None,
        update: bool = False,
    ) -> Dict[str, np.ndarray]:
        """
        Generate prediction intervals and, if ground truth (`y_gt`) is provided, optionally return outlier flags and nonconformity p-values.

        If `update` is enabled, the method calls `predict_batch` with the `update=True` flag every `self.online_size` samples to update the nonconformity scores.

        Args:
            y_pred (np.ndarray): Predicted values. Shape: (n_samples,forecast_length, num_features).
            y_gt (np.ndarray, optional): Ground truth values. Shape: (n_samples, forecast_length, num_features).
            X (np.ndarray, optional): Input covariates. Shape: (n_samples, n_features).
            timestamps (np.ndarray, optional): Timestamps associated with each predicted value. Shape: (num_samples, forecast_length ).
            false_alarm (float, optional): Desired error rate in [0, 1]. If None, defaults to `self.false_alarm`.
            update (bool, optional): Whether to update the nonconformity scores using `y_gt` if provided. Default is False.

        Returns:
            dict: A dictionary containing:
                - prediction intervals for each forecast step and feature (always).
                - and optionally, outlier flags and nonconformity p-values if `y_gt` is provided.
        """

        output = {}
        if self.nonconformity_score in [s.value for s in NonconformityScores]:  #'absolute_error':
            output["prediction_interval"] = {"y_low": np.zeros_like(y_pred), "y_high": np.zeros_like(y_pred)}

        if y_gt is not None:
            output["outliers"] = np.zeros_like(y_pred)
            output["outliers_scores"] = np.zeros_like(y_pred)

        for ix_h in range(y_pred.shape[1]):
            for ix_f in range(y_pred.shape[2]):
                timestamps_i = None
                if timestamps is not None:
                    timestamps_i = timestamps[:, ix_h, ix_f]

                y_gt_i = None
                if y_gt is not None:
                    y_gt_i = y_gt[:, ix_h, ix_f]

                output_i = self.univariate_wrappers[ix_h, ix_f].predict(
                    y_pred[:, ix_h, ix_f],
                    y_gt=y_gt_i,
                    X=X,
                    timestamps=timestamps_i,
                    false_alarm=false_alarm,
                    update=update,
                )

                if self.nonconformity_score in [s.value for s in NonconformityScores]:  # 'absolute_error':
                    output["prediction_interval"]["y_low"][:, ix_h, ix_f] = output_i["prediction_interval"]["y_low"]
                    output["prediction_interval"]["y_high"][:, ix_h, ix_f] = output_i["prediction_interval"]["y_high"]

                if y_gt is not None:
                    output["outliers"][:, ix_h, ix_f] = output_i["outliers"]
                    output["outliers_scores"][:, ix_h, ix_f] = output_i["outliers_scores"]

        return output


class WeightedConformalWrapper:
    def __init__(
        self,
        nonconformity_score: str = NonconformityScores.ABSOLUTE_ERROR.value,
        false_alarm: float = 0.05,
        weighting: str = Weighting.UNIFORM.value,
        weighting_params: Dict[str, Any] = {},
        threshold_function: str = ThresholdFunction.WEIGHTING.value,
        window_size: Optional[int] = None,
        online_adaptive: bool = False,
        online_size: Optional[int] = 1,
    ):
        """
        Weighted Split Conformal Univariate Wrapper Class.

        Args:
            nonconformity_score (str): Type of nonconformity score to use, as defined in the `NonconformityScores` enum.
            false_alarm (float): Desired false alarm (error) rate for the prediction intervals.
            weighting (str): Weighting strategy for the nonconformity scores, as defined in the `Weighting` enum.
            weighting_params (dict): Dictionary of parameters for the weighting method, if applicable.
            threshold_function (str): Method used to compute the threshold, as defined in the `ThresholdFunction` enum.
            window_size (int, optional): Maximum number of past nonconformity scores to consider for calibration.
            online_adaptive (bool): Whether the method uses an online/adaptive update strategy.
            online_size (int, optional): Stride or update frequency for online adaptation.
        """
        self.nonconformity_score = nonconformity_score
        assert self.nonconformity_score in [
            s.value for s in NonconformityScores
        ], "Selected nonconformity score is not supported, choose from {}".format(
            str([s.value for s in NonconformityScores])
        )
        # self.nonconformity_score_func = nonconformity_score_functions

        self.quantile = 1 - false_alarm
        self.false_alarm = false_alarm

        self.weighting = weighting
        self.weighting_params = weighting_params
        self.window_size = window_size
        self.online_size = online_size
        self.online = online_adaptive

        self.threshold_function = threshold_function

        self.cal_scores = []
        self.weights = []
        self.cal_X = []
        self.cal_timestamps = []

        self.score_threshold = None

    def to_dict(self) -> Dict[str, Any]:
        """
        Serializes this instance to a Python dictionary.

        Returns:
            `Dict[str, Any]`: Dictionary of all the attributes that make up this object.
        """
        output = {
            "nonconformity_score": self.nonconformity_score,
            "false_alarm": self.false_alarm,
            "weighting": self.weighting,
            "weighting_params": self.weighting_params,
            "window_size": self.window_size,
            "online_adaptive": self.online,
            "online_size": self.online_size,
            "threshold_function": self.threshold_function,
            "cal_scores": self.cal_scores,
            "weights": self.weights,
            "cal_X": self.cal_X,
            "cal_timestamps": self.cal_timestamps,
            "score_threshold": self.score_threshold,
        }

        return output

    @classmethod
    def from_dict(cls, params: Dict[str, Any], **kwargs) -> "WeightedConformalWrapper":
        """
        Instantiates a type of [`~WeightedConformalWrapper`] from a Python dictionary of
        parameters.

        Args:
            dict (`Dict[str, Any]`):
                Dictionary that will be used to instantiate the object.
            kwargs (`Dict[str, Any]`):
                Additional parameters from which to initialize the object.

        Returns:
            [`~WeightedConformalWrapper`]: The WeightedcCnformalWrapper object instantiated from those
            parameters.
        """

        params_copy = params.copy()

        save_attrs = ["cal_scores", "weights", "cal_X", "cal_timestamps", "score_threshold"]
        save_attrs_dict = {}
        for attr in save_attrs:
            save_attrs_dict[attr] = params_copy.pop(attr)

        obj = cls(**params_copy, **kwargs)

        for attr_name, attr_value in save_attrs_dict.items():
            if isinstance(attr_value, list):
                setattr(obj, attr_name, np.asarray(attr_value))
            else:
                setattr(obj, attr_name, attr_value)

        return obj

    def fit(
        self,
        y_cal_gt: np.ndarray,
        y_cal_pred: np.ndarray,
        X_cal: Optional[np.ndarray] = None,
        cal_timestamps: Optional[np.ndarray] = None,
    ):
        """
        Fit the Weighted Split Conformal method.

        Args:
            y_cal_gt (np.ndarray): Ground truth values. Shape: (num_samples).
            y_cal_pred (np.ndarray): Forecasted values from the time series model. Shape: (num_samples).
            X_cal (np.ndarray, optional): Input covariates used for input-dependent conformal methods. Shape: (num_samples, num_covariates).
            cal_timestamps (np.ndarray, optional): Timestamps corresponding to each forecasted value. Shape: (num_samples).
        """

        # check and remove nans
        filter = ~np.isnan(y_cal_gt) & ~np.isnan(y_cal_pred)
        y_cal_gt = y_cal_gt[filter]
        y_cal_pred = y_cal_pred[filter]

        if self.window_size is None:
            self.window_size = y_cal_pred.shape[0]
        self.cal_scores = nonconformity_score_functions(
            y_cal_gt, y_cal_pred, X=X_cal, nonconformity_score=self.nonconformity_score
        )

        self.cal_scores = self.cal_scores[-self.window_size :]

        if X_cal is not None:
            self.cal_X = X_cal[filter][-self.window_size :]

        if cal_timestamps is not None:
            self.cal_timestamps = cal_timestamps[filter][-self.window_size :]

        critical_efficient_size = int(np.ceil(1 / self.false_alarm))

        # Certain Weighting Methods May Require Fitting
        if self.weighting in [
            Weighting.UNIFORM.value,
            Weighting.EXPONENTIAL_DECAY.value,
        ]:  # ["uniform", "exponential_decay"]:
            cal_weights = self.get_weights()
            self.weights.append(cal_weights)
            # self.weights.append(cal_weights[-self.cal_scores.shape[0]:])
            if self.threshold_function == ThresholdFunction.WEIGHTING.value:  #  "weighting":
                self.score_threshold = self.score_threshold_func(cal_weights, false_alarm=self.false_alarm)
        assert (
            np.sum(cal_weights) >= critical_efficient_size
        ), " The effective size is too small for the desired false alarm of {}, the calibration set should be larger than {} ".format(
            self.false_alarm, critical_efficient_size
        )

    def get_weights(
        self,
        y_pred: Optional[np.ndarray] = None,
        X: Optional[np.ndarray] = None,
        timestamps: Optional[np.ndarray] = None,
        false_alarm: Optional[float] = None,
    ) -> np.ndarray:
        """
        Return the nonconformity calibration weights.

        Args:
            y_pred (np.ndarray, optional): Predicted values. Shape: (num_samples,).
            X (np.ndarray, optional): Input covariates for input-dependent conformal methods. Shape: (num_samples, num_covariates).
            timestamps (np.ndarray, optional): Timestamps associated with each predicted value. Shape: (num_samples,).
            false_alarm (float, optional): Desired error rate (false alarm rate) for the prediction interval.

        Returns:
            np.ndarray: Array of calibration weights. Shape is either (window_size,) or (num_samples, window_size), depending on the weighting strategy.
        """

        if false_alarm is None:
            false_alarm = self.false_alarm

        if self.weighting in [
            Weighting.UNIFORM.value,
            Weighting.EXPONENTIAL_DECAY.value,
        ]:  # ["uniform", "exponential_decay"]:
            if len(self.weights) > 0:
                return self.weights[-1]
            else:
                if self.weighting == Weighting.UNIFORM.value:
                    # return np.ones(y_pred.shape[0],self.cal_scores.shape[0])
                    # return np.ones(self.cal_scores.shape[0])
                    return np.ones(self.window_size)

                if self.weighting == Weighting.EXPONENTIAL_DECAY.value:
                    decay_param = self.weighting_params.get("decay_param", 0.99)
                    return decay_param ** (self.window_size - np.arange(self.window_size))

    def score_threshold_func(
        self,
        cal_weights,
        cal_scores: Optional[np.ndarray] = None,
        y_pred: Optional[np.ndarray] = None,
        X: Optional[np.ndarray] = None,
        timestamps: Optional[np.ndarray] = None,
        false_alarm: Optional[float] = None,
    ) -> Union[float, List[float]]:
        """
        Compute the nonconformity score threshold corresponding to the desired false alarm (error) rate.

        Args:
            cal_weights (np.ndarray): Calibration weights. Can be either:
                - 1D array of shape (num_calibration_scores,)
                - 2D array of shape (num_samples, num_calibration_scores)
            cal_scores (np.ndarray, optional): Array of nonconformity scores used for threshold computation. If not provided, `self.cal_scores` is used.
            y_pred (np.ndarray, optional): Predicted values. Shape: (num_samples,).
            X (np.ndarray, optional): Input covariates for input-dependent thresholding. Shape: (num_samples, num_covariates).
            timestamps (np.ndarray, optional): Timestamps associated with each predicted value. Shape: (num_samples,).
            false_alarm (float, optional): Desired false alarm (error) rate for the prediction interval.

        Returns:
            Union[float, List[float]]: Threshold value(s) for the nonconformity scores, used to determine the prediction intervals.
        """

        if cal_scores is None:
            cal_scores = self.cal_scores

        if false_alarm is None:
            false_alarm = self.false_alarm

        score_threshold = []
        n_cal_scores = cal_scores.shape[0]
        min_required = np.ceil(1 / false_alarm)
        if n_cal_scores < min_required:
            raise ValueError(f"""There are not enough calibration scores for error rate of {false_alarm}.
                             {n_cal_scores} provied but we require {min_required}, try increasing the
                             the number of rows of calibration data.""")
        if self.threshold_function == ThresholdFunction.WEIGHTING.value:  # "weighting":
            if self.nonconformity_score in [s.value for s in PositiveNonconformityScores]:
                if len(cal_weights.shape) == 1:  # same weights for all y
                    score_threshold = weighted_conformal_quantile(
                        np.append(cal_scores, np.array([np.inf]), axis=0),
                        np.append(cal_weights[-n_cal_scores:], np.array([1]), axis=0),
                        alpha=false_alarm,
                    )
                else:
                    for i in range(cal_weights.shape[0]):
                        score_threshold_i = weighted_conformal_quantile(
                            np.append(cal_scores, np.array([np.inf]), axis=0),
                            np.append(cal_weights[i, -n_cal_scores:], np.array([1]), axis=0),
                            alpha=false_alarm,
                        )
                        score_threshold.append(score_threshold_i)
                    score_threshold = np.array(score_threshold)

            elif self.nonconformity_score == NonconformityScores.ERROR.value:
                if len(cal_weights.shape) == 1:  # same weights for all y
                    cal_scores_infty = np.append(cal_scores, np.array([np.inf, -np.inf]), axis=0)
                    # cal_scores_infty = np.append(np.array([]),cal_scores_infty,axis=0)

                    cal_weights_infty = np.append(cal_weights[-n_cal_scores:], np.array([1, 1]), axis=0)
                    # cal_weights_infty = np.append(np.array([1]),cal_weights_infty,axis=0)

                    score_threshold_low = weighted_conformal_quantile(
                        cal_scores_infty, cal_weights_infty, alpha=np.maximum(false_alarm / 2, 1 - (false_alarm / 2))
                    )

                    score_threshold_up = weighted_conformal_quantile(
                        cal_scores_infty, cal_weights_infty, alpha=np.minimum(false_alarm / 2, 1 - (false_alarm / 2))
                    )
                    # if score_threshold_up >= score_threshold_low:
                    # print('SCORE UP LOWER THAN SCORE LOW!!')
                    # print('FALSE ALARM :: ', false_alarm, ' cal scores/weights size :', cal_scores_infty.shape[0], cal_weights_infty.shape[0])
                    # print('MAX/MIN/MEAN scores :', np.mean(cal_scores), np.min(cal_scores), np.max(cal_scores))
                    # print(np.maximum(false_alarm / 2, 1 - (false_alarm / 2)), np.minimum(false_alarm / 2, 1 - (false_alarm / 2)))
                    # print(score_threshold_up,score_threshold_low)
                    assert (
                        score_threshold_up >= score_threshold_low
                    ), " score_threshold_up is not greater than score_threshold_low"

                    score_threshold = [score_threshold_low, score_threshold_up]

                else:
                    for i in range(cal_weights.shape[0]):
                        cal_scores_infty = np.append(cal_scores, np.array([np.inf, -np.inf]), axis=0)
                        # cal_scores_infty = np.append(np.array([-np.inf]),cal_scores_infty,axis=0)

                        cal_weights_infty = np.append(cal_weights[i, -n_cal_scores:], np.array([1, 1]), axis=0)
                        # cal_weights_infty = np.append(np.array([1]),cal_weights_infty,axis=0)

                        # score_threshold_i = weighted_conformal_quantile(np.append(cal_scores,np.array([np.inf]),axis=0),
                        #                     np.append(cal_weights[i,-n_cal_scores:],np.array([1]),axis=0),
                        #                     alpha=false_alarm)

                        score_threshold_low_i = weighted_conformal_quantile(
                            cal_scores_infty,
                            cal_weights_infty,
                            alpha=np.maximum(false_alarm / 2, 1 - (false_alarm / 2)),
                        )

                        score_threshold_up_i = weighted_conformal_quantile(
                            cal_scores_infty,
                            cal_weights_infty,
                            alpha=np.minimum(false_alarm / 2, 1 - (false_alarm / 2)),
                        )
                        score_threshold_i = [score_threshold_low_i, score_threshold_up_i]
                        assert (
                            score_threshold_up_i >= score_threshold_low_i
                        ), " score_threshold_up is not greater than score_threshold_low"

                        score_threshold.append(score_threshold_i)
                    score_threshold = np.array(score_threshold).transpose()  # first dimension is low/up score

            return score_threshold

    def predict_batch(
        self,
        y_pred: np.ndarray,
        y_gt: Optional[np.ndarray] = None,
        X: Optional[np.ndarray] = None,
        timestamps: Optional[np.ndarray] = None,
        false_alarm: Optional[float] = None,
        update: Optional[bool] = None,
    ) -> Dict[str, np.ndarray]:
        """
        Generate prediction intervals for a batch of data and, if ground truth (`y_gt`) is provided, optionally return outliers and nonconformity p-values.

        Args:
            y_pred (np.ndarray): Predicted values. Shape: (n_samples, 1).
            y_gt (np.ndarray, optional): Ground truth values. Shape: (n_samples, 1).
            X (np.ndarray, optional): Input covariates. Shape: (n_samples, n_features).
            false_alarm (float, optional): Desired error rate in [0, 1]. If None, defaults to `self.false_alarm`.
            update (bool, optional): Whether to update the nonconformity scores using `y_gt` if provided. Default is False.

        Returns:
            dict: A dictionary containing:
                - prediction intervals (always),
                - and optionally, outlier flags and nonconformity p-values if `y_gt` is provided.
        """

        if false_alarm is None:
            false_alarm = self.false_alarm

        if update is None:
            update = self.online

        # Weight Computation
        cal_weights = self.get_weights(y_pred, X=X, timestamps=timestamps, false_alarm=false_alarm)[
            -self.cal_scores.shape[0] :
        ]

        # Score Threshold
        if (
            (false_alarm == self.false_alarm)
            and (self.weighting in [Weighting.UNIFORM.value])
            and (self.threshold_function in [ThresholdFunction.WEIGHTING.value])
        ):
            score_threshold = self.score_threshold
        else:
            score_threshold = self.score_threshold_func(
                cal_weights, y_pred=y_pred, X=X, timestamps=timestamps, false_alarm=false_alarm
            )

        # Prediction Interval
        prediction_interval = conformal_set(y_pred, score_threshold, nonconformity_score=self.nonconformity_score)

        output = {}
        if y_gt is not None:
            # Compute nonconformity scores of input
            test_scores = nonconformity_score_functions(
                y_gt, y_pred, X=X, nonconformity_score=self.nonconformity_score
            )
            # print('TEST SCORES :', test_scores.shape, test_scores, score_threshold)

            # Outlier Flag and Outlier Scores
            test_outliers = None
            test_ad_scores = []
            if self.nonconformity_score in [NonconformityScores.ABSOLUTE_ERROR.value]:
                test_outliers = np.array(test_scores > score_threshold).astype("int")
                for score in test_scores:
                    ad_score = weighted_conformal_alpha(
                        np.append(self.cal_scores, np.array([np.inf]), axis=0),
                        np.append(cal_weights, np.array([1]), axis=0),
                        score,
                    )
                    # test_ad_scores.append(1 - ad_score)
                    test_ad_scores.append(ad_score)  # p-value (significance)
            elif self.nonconformity_score in [NonconformityScores.ERROR.value]:
                test_outliers = np.array(
                    (test_scores < score_threshold[0]) | (test_scores > score_threshold[1])
                ).astype("int")
                for score in test_scores:
                    ad_score = weighted_conformal_alpha(
                        np.append(self.cal_scores, np.array([np.inf, -np.inf]), axis=0),
                        np.append(cal_weights, np.array([1, 1]), axis=0),
                        score,
                    )
                    # test_ad_scores.append(1 - ad_score)
                    # test_ad_scores.append(ad_score)  # p-value (significance)
                    test_ad_scores.append(np.minimum(float(ad_score), 1 - float(ad_score)))

            # Update
            if update:
                self.update(test_scores, X=X, timestamps=timestamps)
            output["outliers"] = test_outliers
            output["outliers_scores"] = np.array(test_ad_scores).flatten()

        output["prediction_interval"] = prediction_interval
        return output

    def predict(
        self,
        y_pred,
        y_gt: Optional[np.ndarray] = None,
        X: Optional[np.ndarray] = None,
        timestamps: Optional[np.ndarray] = None,
        false_alarm: Optional[float] = None,
        update: Optional[bool] = None,
    ) -> Dict[str, np.ndarray]:
        """
        Generate prediction intervals and, if ground truth (`y_gt`) is provided, optionally return outlier flags and nonconformity p-values.

        If `update` is enabled, the method calls `predict_batch` with the `update=True` flag every `self.online_size` samples to update the nonconformity scores.

        Args:
            y_pred (np.ndarray): Predicted values. Shape: (n_samples, 1).
            y_gt (np.ndarray, optional): Ground truth values. Shape: (n_samples, 1).
            X (np.ndarray, optional): Input covariates. Shape: (n_samples, n_features).
            false_alarm (float, optional): Desired error rate in [0, 1]. If None, defaults to `self.false_alarm`.
            timestamps (np.ndarray, optional): Timestamps associated with each predicted value. Shape: (num_samples,).
            update (bool, optional): Whether to update the nonconformity scores using `y_gt` if provided. Default is False.

        Returns:
            dict: A dictionary containing:
                - prediction intervals (always),
                - and optionally, outlier flags and nonconformity p-values if `y_gt` is provided.
        """

        if false_alarm is None:
            false_alarm = self.false_alarm

        if update is None:
            update = self.online

        n_samples = y_pred.shape[0]
        n_batches = int(np.ceil(n_samples / self.online_size))

        if (y_gt is not None) and update:
            """
            Batch Inference
            """
            output = None
            for ix_b in range(n_batches):
                ix_ini = int(ix_b * self.online_size)
                ix_end = np.minimum(int(ix_b * self.online_size + self.online_size), y_pred.shape[0])

                y_pred_b = y_pred[ix_ini:ix_end]
                y_gt_b = y_gt[ix_ini:ix_end]
                X_b = None
                if X is not None:
                    X_b = X[ix_ini:ix_end]
                timestamps_b = None
                if timestamps is not None:
                    timestamps_b = timestamps[ix_ini:ix_end]
                output_b = self.predict_batch(
                    y_pred_b, y_gt=y_gt_b, X=X_b, timestamps=timestamps_b, false_alarm=false_alarm, update=update
                )
                if output is None:
                    output = output_b.copy()
                else:
                    for k in output_b.keys():
                        if k == "prediction_interval":
                            for k2 in output_b[k].keys():
                                output[k][k2] = np.append(output[k][k2], np.array(output_b[k][k2]), axis=0)
                        else:
                            output[k] = np.append(output[k], np.array(output_b[k]), axis=0)
        else:
            output = self.predict_batch(
                y_pred, y_gt=y_gt, X=X, timestamps=timestamps, false_alarm=false_alarm, update=update
            )

        return output

    def predict_interval(
        self,
        y_pred: np.ndarray,
        X: Optional[np.ndarray] = None,
        timestamps: Optional[np.ndarray] = None,
        false_alarm: Optional[float] = None,
    ) -> Dict[str, np.ndarray]:
        """
        Generate prediction intervals. (no update)

        Args:
            y_pred (np.ndarray): Predicted values. Shape: (n_samples, 1).
            X (np.ndarray, optional): Input covariates. Shape: (n_samples, n_features).
            timestamps (np.ndarray, optional): Timestamps associated with each predicted value. Shape: (num_samples,).
            false_alarm (float, optional): Desired error rate in [0, 1]. If None, defaults to `self.false_alarm`.

        Returns:
            dict: A dictionary containing prediction intervals
        """

        if false_alarm is None:
            false_alarm = self.false_alarm

        # Weight Computation
        cal_weights = self.get_weights(y_pred, X=X, timestamps=timestamps, false_alarm=false_alarm)

        # Score Threshold
        score_threshold = self.score_threshold_func(
            cal_weights, y_pred=y_pred, X=X, timestamps=timestamps, false_alarm=false_alarm
        )

        # Prediction Interval
        prediction_interval = conformal_set(y_pred, score_threshold, nonconformity_score=self.nonconformity_score)

        return prediction_interval

    def update(self, scores: np.ndarray, X: Optional[np.ndarray] = None, timestamps: Optional[np.ndarray] = None):
        """
        Update the nonconformity scores and threshold function.

        Args:
            scores (np.ndarray): Nonconformity scores. Shape: (n_samples, 1).
            X (np.ndarray, optional): Input covariates for input-dependent methods. Shape: (n_samples, n_features).
            timestamps (np.ndarray, optional): Timestamps associated with each predicted value. Shape: (n_samples,).
        """

        self.cal_scores = np.append(self.cal_scores, scores, axis=0)
        self.cal_scores = self.cal_scores[-self.window_size :]

        if timestamps is not None:
            self.cal_timestamps.extend(timestamps)
            self.cal_timestamps = self.cal_timestamps[-self.window_size :]
        if X is not None:
            self.cal_X = np.append(self.cal_X, X, axis=0)
            self.cal_X = self.cal_X[-self.window_size :]

        if self.weighting == Weighting.UNIFORM.value:  # "uniform":
            cal_weights = self.get_weights()
            if self.threshold_function == ThresholdFunction.WEIGHTING.value:
                self.score_threshold = self.score_threshold_func(cal_weights, false_alarm=self.false_alarm)


def weighted_conformal_quantile(
    scores: np.ndarray,
    weights: np.ndarray,
    alpha: float = 0.05,
    conformal_correction: bool = False,
    max_score: float = np.inf,
) -> float:
    """
    Predict the weighted conformal quantile.

    Args:
        scores (np.ndarray): Nonconformity scores. Shape: (n,).
        weights (np.ndarray): Weights corresponding to each score. Shape: (n,).
        alpha (float): Significance level for the quantile (e.g., 0.05 for a 95% prediction interval).
        conformal_correction (bool): Whether to apply conformal quantile correction.
        max_score (float): Maximum allowable value for the quantile.

    Returns:
        float: Estimated weighted conformal quantile.
    """

    if weights is None:
        weights = np.ones_like(scores)

    assert np.max(weights) <= 1, "Maximum weight needs to be smaller or equal than 1"
    assert np.min(weights) >= 0, "Minimum weight needs to be greater or equal to 0"
    assert weights.shape[0] == scores.shape[0], " Scores shape does not match weights shape"

    """Add infinite score to the score list"""
    if conformal_correction:
        weights = np.append(weights, np.array([1]))
        scores = np.append(scores, np.array([max_score]))

    # Ensure weights sum to 1
    weights = np.array(weights) / np.sum(weights)

    # Sort the residuals and corresponding weights
    sorted_indices = np.argsort(scores)
    sorted_scores = scores[sorted_indices]
    sorted_weights = weights[sorted_indices]

    # Compute cumulative sum of weights
    cumulative_weights = np.cumsum(sorted_weights)

    # Find the smallest residual where the cumulative weight exceeds 1 - alpha
    quantile_index = np.searchsorted(cumulative_weights, 1 - alpha)
    conformal_quantile = sorted_scores[quantile_index]

    return conformal_quantile


def weighted_conformal_alpha(
    scores: np.ndarray,
    weights: np.ndarray,
    score_observed: np.ndarray,
    conformal_correction: bool = False,
    max_score: float = np.inf,
) -> float:
    """
    Compute the weighted conformal p-value.

    Args:
        scores (np.ndarray): Calibration or previously observed nonconformity scores. Shape: (n,).
        score_observed (float): Nonconformity score of the test (new) observation.
        weights (np.ndarray): Weights for each calibration score. Shape: (n,).
        conformal_correction (bool): Whether to apply conformal correction (e.g., additive smoothing).
        max_score (float): Maximum allowable value for scores (used for clipping or normalization).

    Returns:
        float: Weighted conformal p-value.
    """

    if weights is None:
        weights = np.ones_like(score_observed)

    """Add infinite score to the score list"""
    if conformal_correction:
        weights = np.append(weights, np.array([1]))
        scores = np.append(scores, np.array([max_score]))

    weights = np.array(weights) / np.sum(weights)

    # Sort the residuals and corresponding weights
    sorted_indices = np.argsort(scores)
    sorted_scores = scores[sorted_indices]
    sorted_weights = weights[sorted_indices]

    smaller_scores = sorted_scores > score_observed

    return np.sum(sorted_weights[smaller_scores])


"""
Adaptive Weight Class
"""


class AdaptiveWeightedConformalScoreWrapper:
    def __init__(
        self,
        false_alarm: float = 0.05,
        window_size: Optional[int] = None,
        weighting: str = Weighting.UNIFORM.value,
        weighting_params: Dict[str, Any] = {},
    ):  # , threshold_function = 'weighting'):
        """
        Adaptive Weighted Conformal Score Class.

        Args:
            window_size (int, optional): Maximum number of past nonconformity scores to consider for calibration.
            false_alarm (float, optional): Desired false alarm (error) rate for the prediction intervals.
            weighting (str, optional): Weighting strategy for the nonconformity scores, as defined in the `Weighting` enum.
            weighting_params (dict, optional): Dictionary of parameters for the weighting method, if applicable.
                Supported keys include:
                - 'lr' (float, optional): Learning rate for weight optimization.
                - 'n_batch_update' (int, optional): Number of consecutive observations used to compute the empirical CDF for the Wasserstein-1 distance loss.
                - 'stride' (int, optional): Number of new observations between successive optimization updates.
                - 'epochs' (int, optional): Number of optimization steps performed on the same data batch.
                - 'conformal_weights_update' (bool, optional): Whether to include conformal correction when computing the Wasserstein-1 distance loss for optimization. (Recommended: False for most use cases.)
        """

        self.weighting = weighting
        self.weighting_params = weighting_params
        self.weights_optimizer = None  # initializaed in fit
        self.losses = []

        self.cal_scores = []
        self.cal_weights = []

        self.false_alarm = false_alarm
        self.weights_critical_norm = np.ceil(1 / self.false_alarm - 1)

        self.window_size = (
            window_size if window_size is not None else int(np.maximum(5000, self.weights_critical_norm))
        )  # large memory
        if self.window_size is not None:
            assert (
                self.window_size > self.weights_critical_norm
            ), "Given the false alarm window size must be larger than {}".format(np.ceil(self.weights_critical_norm))

        if "lr" not in self.weighting_params.keys():
            self.weighting_params["lr"] = 0.001
        if "n_batch_update" not in self.weighting_params.keys():
            self.weighting_params["n_batch_update"] = int(self.weights_critical_norm * 4)
        if "stride" not in self.weighting_params.keys():
            self.weighting_params["stride"] = int(1)
        if "epochs" not in self.weighting_params.keys():
            self.weighting_params["epochs"] = int(2)
        if "conformal_weights_update" not in self.weighting_params.keys():
            self.weighting_params["conformal_weights_update"] = False
        if "prior_past_weights_value" not in self.weighting_params.keys():
            self.weighting_params["prior_past_weights_value"] = "proximity"

        self.weights_average = None
        self.weights_average_count = 0

        # assert (
        #     self.weighting_params["n_batch_update"] > self.weights_critical_norm
        # ), "Given the false alarm n_batch_update must be larger than {}".format(np.ceil(self.weights_critical_norm))

    def fit(self, scores: np.ndarray) -> np.ndarray:
        """
        Fit the model using calibration nonconformity scores.

        Args:
            scores (np.ndarray): Nonconformity calibration scores. Shape: (n_samples,).
        """

        """
        1. Add Calibration Scores
        """
        assert scores.shape[0] > self.weights_critical_norm, "Provide a minimum of {} scores for fitting".format(
            self.weights_critical_norm
        )
        self.cal_scores = torch.tensor(scores)
        if self.window_size is None:
            self.window_size = scores.shape[0]

        ## Initialize Weights + Optimizer
        w_init = torch.zeros(self.window_size)
        w_init[-scores.shape[0] :] = 1

        if torch.cuda.is_available():
            self.device = torch.device("cuda")
        else:
            self.device = torch.device("cpu")

        self.weights_parameters = torch.nn.Parameter(w_init.to(self.device))
        self.weights_optimizer = torch.optim.AdamW([self.weights_parameters], lr=self.weighting_params["lr"])
        # self.weights_optimizer = torch.optim.RMSprop([self.weights_parameters], lr=self.weighting_params["lr"])

        """
        2. Compute p-values for the given scores (should be ignored)
        """

        # Expand to nxn by repeating along the row dimension
        torch_scores_past = self.cal_scores.unsqueeze(0).repeat(scores.shape[0], 1)  # Shape: (n, n)

        with torch.no_grad():
            betas = (
                get_beta(
                    self.cal_scores.to(self.device),
                    torch_scores_past.to(self.device),
                    weights=self.weights_parameters[-scores.shape[0] :],
                    conformal_weights=True,
                )
                .clone()
                .detach()
                .cpu()
                .numpy()
            )

        return 1 - np.array(betas)

    def predict(self, scores: np.ndarray, verbose: bool = False) -> np.ndarray:
        """
        Perform online prediction of conformal p-values for observed scores.

        This method updates the weighting parameters in an online fashion based on `self.weighting` and `self.weights_parameters`.

        Args:
            scores (np.ndarray): Observed nonconformity scores. Shape: (n_samples,).
            verbose (bool, optional): If True, enables verbose output. Default is False.


        Returns:
            np.ndarray: Array of weighted p-scores (beta_t) for each test score. Shape: (n_samples,).
        """

        # print('scores :', scores.shape[0])

        n_scores = scores.shape[0]
        n_batch = int(self.weighting_params["n_batch_update"])
        n_epochs = int(self.weighting_params["epochs"])
        stride = int(self.weighting_params["stride"])
        conformal_weights_update = self.weighting_params["conformal_weights_update"]

        n_min_update = int(
            self.weights_critical_norm + n_batch
        )  # Minimum previous scores needed to update weights with Wass1

        # n_batch_updates = (n_scores - n_batch) / stride + 1
        n_batch_updates = n_scores / stride  # + 1
        n_batch_updates = int(np.ceil(n_batch_updates))

        losses = []
        beta_output = []
        for i in range(n_batch_updates):
            ini_t_i = int(i * stride)
            end_t_i = np.minimum(int(i * stride + stride), scores.shape[0])
            if verbose:
                LOGGER.info("ini/end batch :", ini_t_i, end_t_i)
            scores_new_i = torch.tensor(scores[ini_t_i:end_t_i])
            if len(scores_new_i) == 0:
                continue
            """Generate Matrix With Past Scores per batch observation and "present" scores to compute Wass1"""
            scores_concat_i = torch.cat((self.cal_scores, scores_new_i), dim=0)
            update_weights_feasible = len(scores_concat_i) >= n_min_update
            n_batch_effective = n_batch if update_weights_feasible else scores_new_i.shape[0]
            n_batch_effective = int(n_batch_effective)
            # print(ini_t_i,end_t_i, scores_concat_i.shape, n_batch_effective, self.cal_scores.shape[0], self.weights_critical_norm, n_min_update)
            # print('Update feasible :', update_weights_feasible)
            scores_i = scores_concat_i[-n_batch_effective:].to(self.device)  # Scores to compute the Wass1 distance
            cal_context_available = len(scores_concat_i) - n_batch_effective
            scores_matrix = [
                scores_concat_i[j - cal_context_available : j]
                for j in range(len(scores_concat_i) - n_batch_effective, len(scores_concat_i))
            ]

            scores_matrix = torch.stack(scores_matrix).to(self.device)
            # print(scores_matrix.shape)

            """Get Betas"""
            if self.weights_average is None:
                self.weights_average = self.weights_parameters.clone().detach()
                self.weights_average_count = 1

            with torch.no_grad():
                n_scores_compute = int(scores_new_i.shape[0])
                # print("n_scores_compute matrix :", scores_matrix.shape)
                # print("cal shape :", self.cal_scores.shape[0])
                betas = (
                    get_beta(
                        scores_i[-n_scores_compute:],
                        scores_matrix[-n_scores_compute:],
                        weights=self.weights_parameters[-scores_matrix.shape[1] :],
                        # weights=self.weights_average[-scores_matrix.shape[1]:]/self.weights_average_count,
                        conformal_weights=True,
                    )
                    .clone()
                    .detach()
                    .cpu()
                    .numpy()
                )
                if verbose:
                    LOGGER.info("Beta shape :", betas.shape)
                beta_output.extend(betas.tolist())
                # print('beta length :', len(beta_output), '; new scores shapes :', scores_i[-n_scores_compute:].shape)

            """Optimize Weights"""
            # if not update_weights_feasible:

            if update_weights_feasible:
                # print("n_scores_compute matrix :", scores_matrix[-n_scores_compute:].shape)
                # print("cal shape :", self.cal_scores.shape[0])
                losses_i = []
                for _ in range(n_epochs):
                    self.weights_optimizer.zero_grad()
                    loss = get_w1_distance(
                        scores_i,
                        scores_matrix,
                        weights=self.weights_parameters[-scores_matrix.shape[1] :],
                        conformal_weights=conformal_weights_update,
                    )
                    losses_i.append(loss.detach().item())

                    loss.backward()
                    self.weights_optimizer.step()

                    ## Project weights into the simplex
                    # with torch.no_grad():
                    #     self.weights_parameters.data = project_l1_box_torch(
                    #         self.weights_parameters.data, min_l1_norm=self.weights_critical_norm, max_value=1.0
                    #     )
                    with torch.no_grad():
                        self.weights_parameters.copy_(
                            project_l1_box_torch(
                                self.weights_parameters, min_l1_norm=self.weights_critical_norm, max_value=1.0
                            )
                        )
                    if verbose:
                        LOGGER.info(
                            "loss {}, w_norm_l1 {}, w_min {}, w_max {}".format(
                                losses_i[-1],
                                self.weights_parameters.data.sum(),
                                self.weights_parameters.data.min(),
                                self.weights_parameters.data.max(),
                            )
                        )

                    self.weights_average = self.weights_average + self.weights_parameters.clone().detach()
                    self.weights_average_count += 1
                    # self.weights_parameters.data.clamp_(0, 1.0)
                losses.append(losses_i)
                # self.cal_weights = self.weights_average.clone().detach().numpy()/self.weights_average_count
                self.cal_weights = self.weights_parameters.clone().detach().cpu().numpy()
            else:
                if verbose:
                    LOGGER.info(
                        "Update not feasible for indexes {} to {}; all available past scores are {} minimum number required are {} ".format(
                            ini_t_i,
                            end_t_i,
                            len(scores_concat_i),
                            n_min_update,
                        )
                    )
                # print('Update not feasible for indexes :',ini_t_i, end_t_i )
                # print('Cal scores size :', len(self.cal_scores))
                # print('Minimum number of elements required are :', n_min_update, ' but in total we have :', scores_concat_i.shape[0])
            """ Update Past Calibration Scores """
            self.cal_scores = torch.cat((self.cal_scores, scores_new_i[0:stride]), dim=0)

            if self.weighting_params["prior_past_weights_value"] == "proximity":
                n = self.cal_scores.shape[0]
                if n <= self.window_size:
                    # print("BEFORE")
                    # print(self.weights_parameters.data[-n:].shape)
                    # print(n_batch_effective)
                    # print(self.weights_parameters.data[-n : -n + n_batch_effective + 10])
                    # self.weights_parameters.data[0 : -int(self.cal_scores.shape[0] + 1)] = (
                    #     self.weights_parameters.data[-int(self.cal_scores.shape[0] + 1)]
                    # )
                    with torch.no_grad():
                        self.weights_parameters[-n : -n + n_batch_effective].copy_(
                            self.weights_parameters[-n + n_batch_effective]
                        )
                    # print("AFTER")
                    # print(self.weights_parameters.data[-n :].shape)
                    # print(self.weights_parameters.data[-n : -n + n_batch_effective + 10])

            self.cal_scores = self.cal_scores[-self.window_size :]
            self.cal_weights = self.weights_parameters.clone().detach().cpu().numpy()
            # print('Cal scores updated to :', len(self.cal_scores))
        if len(losses) > 0:
            self.losses.extend(losses)
        return 1 - np.array(beta_output)


"""
Optimization functions
"""


def get_beta(
    test_scores: torch.Tensor,
    observed_scores: torch.Tensor,
    weights: Optional[torch.Tensor] = None,
    conformal_weights: bool = True,
) -> torch.Tensor:
    """
    Compute the weighted conformal p-scores (beta_t) for a batch of test scores given observed calibration scores.

    Args:
        test_scores (np.ndarray): Array of test (observed) nonconformity scores. Shape: (b,).
        observed_scores (np.ndarray): Array of previously observed scores. Shape: (b, w), where each row contains the calibration scores for a given test point.
        weights (np.ndarray): Array of time-based weights applied to observed scores. Shape: (w,). All weights should lie in the range [0, 1].
        conformal_weights (bool, optional): Apply conformal quantile conservative correction.

    Returns:
        np.ndarray: Array of weighted p-scores (beta_t) for each test score. Shape: (b,).
    """

    b, w = observed_scores.shape

    assert (
        test_scores.ndim == 1 and test_scores.shape[0] == b
    ), "incorrect dimensions for test and observed scores, got {} and {}".format(
        test_scores.shape, observed_scores.shape
    )

    if weights is None:
        weights = torch.ones(w)

    assert (
        weights.ndim == 1 and weights.shape[0] == w
    ), "incorrect dimensions for weights and observed scores, got {} and {}".format(
        weights.shape, observed_scores.shape
    )
    assert (
        weights.max() <= 1 and weights.min() >= 0
    ), "weights should be constrained between 0 and 1, got {} and {}".format(weights.max(), weights.min())

    betas = (weights.unsqueeze(0) * (observed_scores <= test_scores.unsqueeze(1))).sum(1)

    weight_norm = weights.sum()
    if conformal_weights:
        weight_norm = weight_norm + 1
        betas = betas + 1
    betas = betas / weight_norm

    # betas = ((weights.unsqueeze(0) * (observed_scores<= test_scores.unsqueeze(1))).sum(1)+1)/weight_norm
    return betas


def w1_distance_from_betas(beta: torch.Tensor) -> torch.Tensor:
    """
    Compute the 1-Wasserstein distance between the empirical distribution
    given by sorted samples `beta` in [0,1] and the uniform distribution on [0,1].

    Args:
        beta: 1D tensor of shape (N,), sorted (beta[0] <= beta[1] <= ... <= beta[N-1])
              with values in [0,1].

    Returns:
        A scalar tensor representing W1 distance.
    """
    # Number of samples
    N = beta.shape[0]

    # For k = 1..N, define the subinterval [a_k, b_k] = [ (k-1)/N, k/N ]
    k = torch.arange(1, N + 1, dtype=beta.dtype, device=beta.device)
    a = (k - 1) / N  # left endpoints
    b = k / N  # right endpoints

    # ----- Piecewise closed-form integrals -----
    #
    # 1) If beta_k < a_k, then integral = ∫_{a_k}^{b_k} (p - beta_k) dp
    left_val = 0.5 * (b**2 - a**2) - beta * (b - a)

    # 2) If beta_k > b_k, then integral = ∫_{a_k}^{b_k} (beta_k - p) dp
    right_val = beta * (b - a) - 0.5 * (b**2 - a**2)

    # 3) If a_k <= beta_k <= b_k, split at beta_k:
    #    ∫ (|beta_k - p| dp) = (area left of beta_k) + (area right of beta_k)
    #    = beta^2 + 0.5(a^2 + b^2) - beta(a + b)
    middle_val = beta**2 + 0.5 * (a**2 + b**2) - beta * (a + b)

    # ----- Select which formula applies for each i -----
    mask_left = beta < a
    mask_right = beta > b

    val = torch.where(mask_left, left_val, torch.where(mask_right, right_val, middle_val))

    # Summation over i=1..N gives W1 distance
    return val.sum()


def get_w1_distance(test_scores, observed_scores, weights=None, conformal_weights=True) -> float:
    """
    Compute the 1-Wasserstein distance (W1) between the empirical distribution of weighted conformal p-values and the uniform distribution on [0, 1].

    The p-values are estimated using `test_scores` and `observed_scores`, weighted by `weights`.

    Args:
        test_scores (np.ndarray): Test nonconformity scores. Shape: (b,).
        observed_scores (np.ndarray): Previously observed calibration scores. Shape: (b, w), where each row corresponds to calibration scores for a test instance.
        weights (np.ndarray): Time-based weights applied to the calibration scores. Shape: (w,). Values must lie in the range [0, 1].
        conformal_weights (bool, optional): Whether to apply a conservative conformal correction when estimating p-values.

    Returns:
        float: Scalar representing the 1-Wasserstein distance between the empirical p-value distribution and the uniform distribution on [0, 1].
    """

    beta = get_beta(test_scores, observed_scores, weights, conformal_weights=conformal_weights)
    # Sort beta to enforce monotonicity
    beta_sorted = torch.sort(beta)[0]
    return w1_distance_from_betas(beta_sorted)


"""
Weight Projections
"""


def euclidean_proj_simplex_torch(vector: torch.Tensor, radius: int = 1) -> torch.Tensor:
    """Euclidean projection on a positive simplex.
    Algorithm 1 in
    Efficient Projections onto the .1-Ball for Learning in High Dimensions, Duchi et al.
    http://www.cs.berkeley.edu/~jduchi/projects/DuchiSiShCh08.pdf
    """
    assert radius > 0, "radius parameter must be strictly positive"
    n = vector.shape[0]
    # Check if vector is on the simplex
    if torch.isclose(vector.detach().clone().sum(), torch.tensor(radius)) and torch.all(vector >= 0):
        return vector
    v_sorted, _ = torch.sort(vector, descending=True)  # Sort vector in decreasing order
    cum_vector = torch.cumsum(v_sorted, dim=0)
    rho = torch.nonzero(
        v_sorted * torch.arange(1, n + 1, dtype=vector.dtype, device=vector.device) > (cum_vector - radius)
    )[-1]  # Find rho
    theta = (cum_vector[rho] - radius) / (rho + 1)  # Compute theta
    w = torch.clamp(vector - theta, min=0)  # Compute projection
    return w


def project_l1_box_torch(v, min_l1_norm: float = 1, max_value: float = 1, verbose: bool = False) -> torch.Tensor:
    w = torch.clamp(v, min=0, max=max_value)
    if v.sum() > torch.tensor(min_l1_norm):
        return w
    else:
        condition = False
        n_iter = 0
        w_i = torch.clamp(v, max=max_value)  # clamp maximum values
        while not condition:
            w_proj = w_i[w_i < max_value]  # consider only entries that are allowed to grow
            l1_reduce = (
                min_l1_norm - torch.sum(w_i == max_value) * max_value
            )  # adjust l1 norm constrain based on norm of entries that are already in their max values
            if l1_reduce <= 0:
                return torch.clamp(w_i, min=0)

            w_proj = euclidean_proj_simplex_torch(w_proj, radius=l1_reduce)  # project
            w_i[w_i < max_value] = torch.clamp(w_proj, max=max_value, min=0)  # assign

            if torch.sum(w_i) >= torch.tensor(min_l1_norm):
                return w_i
            w_i[w_i == 0] = v[w_i == 0]  ## assign original values to items truncated to zero

            n_iter += 1
            condition = n_iter > 10
            if verbose:
                LOGGER.info("iter : ", n_iter)

        w_i = torch.clamp(w_i, max=max_value, min=0)
        if verbose:
            LOGGER.info(
                "proj ending due to max iterations, norm_l1 is {} and objective was >= {}".format(
                    w_i.sum(), min_l1_norm
                )
            )
        return w_i
